% !TEX root = ../thesis.tex
%
\chapter{Classifications}
\label{sec:classifications}

\section{Storage Solutions}
\label{sec:classifications:storage}

The first technical decision to make is what to use as a storage solution.
I first explain requirements which the solution has to fulfill, then present the candidates, and ultimately evaluate according to these requirements which solution is most appropriate.

\subsection{Requirements}
\label{subsec:classifications:storage:req}

\ac{CSE} requires the storage layer of such a system to have certain capabilities, namely flexibility and scalability.
In addition to basic storage capabilities, the storage solution has to provide some functionalities related to the use case (temporal queries).
I have identified the following requirements for the storage solution, based on which the candidates will be evaluated:

\begin{description}
\item [Basic storage capabilities]
The storage solution must at least provide means of creating and reading persisted data records.
\item [Modify]
In order to facilitate experimentation, the storage solution shall have some means of correcting faulty data records, i.e. an update and delete mechanic.
This is desirable in cases where a change in the software introduced erroneous behavior which has to be undone.
%It would not be sufficient to just spin up a separate instance of the storage solution for the experiment because users should not be aware of whether they are part of the treatment group.
\item [Cascading update]
In addition to a regular modify feature, the storage solution ideally also has some means of correcting all data that is invalid due to being dependent on one invalid data record.
\item [Temporal queries]
It would presumably be useful to be able to retrieve all data records that were created within a specific time frame via some query.
These queries have to be efficient because running an analysis must not affect the storage solution's normal operation performance.
\todo{would this even be used with an aggregation service???}
\todo{do not call this "temporal queries" as the event store already has a concept with that name}
\item [Vertical scalability]
For performance reasons, the storage solution shall be vertically scalable, i.e. scale \emph{up} when more computing power is added.
%This allows have some mechanism for handling large a large amount of requests on the same data.
\item [Horizontal scalability]
The storage solution shall also be able to scale horizontally, i.e. scale \emph{out}, which means that it is able to increase its performance linearly by adding more computing nodes to the network.
This is often solved via \emph{sharding}, where the data is split over multiple logical or physical storage instances, each of them shouldering a portion of the cumulative system load.
\todo{find a more precise definition for scalability, redo these requirements}
%Database clustering is done for various reasons. Clusters can improve availability, fault tolerance, and increase performance by applying a divide and conquer approach as work is distributed over many machines. Clustering is sometimes combined with partitioning and sharding to further break up a large complex task into smaller, more manageable units. 
\item [HTTP interface]
The storage solution shall have a HTTP interface, therefore allowing other services and applications to store and retrieve data via a universally supported medium.
Using HTTP comes with a performance overhead compared to other protocols such as TCP, but as  the advantages in terms of flexibility outweigh this drawback\todo{says who??}; in cases where performance is critical, TCP could still be used if supported.
It should be noted that almost any storage solution can be extended to have an HTTP interface by writing a wrapper in a general purpose programming language that exposes the database functionality.
However, this introduces additional overhead and complexity, as the database wrapper has to be implemented manually -- not to mention potential security issues.
\item [Free License]
The designed system shall be as widely applicable as possible and therefore not impose any financial requirements on the potential applicant.
Thus, the storage solution shall be available free for commercial use, e.g. in form of an open source license such as the GNU GPL\footnote{\url{https://www.gnu.org/licenses/gpl.html}} or Apache License\footnote{\url{https://www.apache.org/licenses/}}.
\end{description}

%I additionally rate storage solution candidates based on maturity, licensing, and applicability.
%Maturity is a rather subjective metric but can be based at least partly on the technology's age and its popularity.
%Licensing could be an issue if the technology has a proprietary license, or if modifications have to be made to the source code (although the latter case is rather unlikely).
%Applicability refers to how well the technology is presumed to fit to the use case of storing, accessing and modifying multiple related user actions.
%\todo{Be more specific - which scale?}

\subsection{Candidates}

Blabla

\begin{enumerate}
\item \ac{SQL} Databases: Blurb about SQL databases.
\begin{enumerate}
\item Postgres
\item MySQL
\item MS-SQL
\item Oracle
\item ...
\end{enumerate}
\item NoSQL Databases: Blurb about NoSQL databases.\cite{strauch2011nosql}
\begin{enumerate}
\item Document stores: Event Store, MongoDb
\item Graph databases: Neo4j
\item Column databases: Cassandra
\end{enumerate}
\item Other: Kafka+X (?)
\end{enumerate}

\subsection{Evaluation}

By analyzing each of the candidates, I determined that Event Store is indeed the most appropriate solution, Neo4j and MongoDb being the most promising alternatives.
Other solutions came short in at least one integral requirement.
See \cref{table:classifications:storage} for the complete results of the analysis.

Using event sourcing as the technique for storing passive user feedback is a natural fit because the use case benefits greatly from the fact that not only the current state is saved, but also how this state was reached.
By storing the events that occur when a user uses the respective application, the complete interaction trace can be computed just by querying the event store.
Modification of the application state in general can be done simply by issuing a new event that reflects this change (e.g. an \texttt{AddressChangedEvent} if a customer's address shall be modified).
If other changes are dependent on this change, it is also possible to make use of the event replay technique by modifying the event that introduced the erroneous change and then replaying all subsequent events.
In terms of scalability, who knows? \todo{research}
Event Store is available under the BSD license and thus free for personal and commercial use.
As it checks off all requirements, Event Store is the ideal candidate for the storage solution of the passive user feedback analysis system.

All \ac{SQL} databases that I initially considered do not fulfill at least one of the mandatory requirements.
While Postgres\footnote{\url{https://www.postgresql.org/}} does not support sharding or other horizontal scaling solutions, MySQL\footnote{\url{https://www.mysql.com/}} and Oracle database\footnote{\url{https://www.oracle.com/database/}} have proprietary license models when used in a commercial context, which makes it difficult to recommend those databases in general.
Additionally, MySQL does not offer an own HTTP interface, which would complicate its implementation even more.
Cascading update is also problematic with \ac{SQL} databases: Although relations between tuples are possible, none of the considered databases have an inherent mechanism for modifying a series of dependent data entries.\todo{not precise enough}

Neo4j\footnote{\url{https://neo4j.com/}} appears to be a decent candidate.
As it is a graph database, it has a more explicit mapping of relations between tuples than \ac{SQL} databases.
This allows for concise update queries which can implement what was termed as "cascading updates" in the requirements.
Although Neo4j does not support clustering in the non-enterprise edition, its authors claim that the graph database nevertheless scales both vertically and horizontally~\cite{...}.
The full featured enterprise edition is not free for commercial use.

MongoDB\footnote{\url{https://www.mongodb.com/}} is another promising candidate.
It is arguably the most popular NoSQL database, especially for its great read and write performance~\cite{6625441,...} as well as scalability~\cite{...}.
MongoDb being a document database means that the data is more loosely coupled than, for example, in a graph database such as Neo4j.
This makes cascading updates harder to implement, but not impossible.
In conclusion, MongoDb is, together with Event Store, the most suitable storage solution for this use case.

Key-value stores such as Apache Ignite\footnote{\url{https://ignite.apache.org/}} are not suitable for this use case.
One reason is that the concept of relations between values is in general not present in key-value stores.
Another reason is that their dynamic data structures do not support temporal queries and cascading updates out of the box.

Cassandra\footnote{\url{https://cassandra.apache.org/}} is strong in terms of performance and scalability.
However, due to its \ac{CQL} being modeled closely to \ac{SQL}, it has the same disadvantages in terms of cascading updates.
Another drawback is that it does not have a built-in HTTP interface.
Cassandra heavily emphasizes and excels in scalability and fault tolerance, but sacrifices read performance for this~\cite{6625441}.
These reasons make Cassandra a great choice for certain specialized use cases, but seems less suitable for a general purpose application and thus it is not used here.

\begin{table}[]
\centering
\caption{Classification of storage solutions}
\begin{tabular}{lllllllllll}
\textbf{Name} & \textbf{Type} & \textbf{Casc. Updates} & \textbf{Sharding} & \textbf{HTTP} & \textbf{License} \\
Event Store & Document & yes & yes & yes & BSD \\
Postgres & SQL & no & no & no & PostgreSQL \\
MySQL & SQL & no & yes & no & GPL / Proprietary \\
Oracle & SQL & no & yes & with addons & Proprietary \\
MongoDB & Document & ? & yes & yes & GNU AGPLv3 \\
Ignite & Key-Value & ? & yes & yes & Apache v2 \\
Neo4j & Document & yes & no & yes & GPL / Proprietary \\
Cassandra & Document & no & yes & no & Apache v2
\end{tabular}
\label{table:classifications:storage}
\end{table}

\section{Data Aggregation Solutions}
\label{sec:classifications:aggregation}

\subsection{Requirements}


\begin{description}
\item [HTTP Interface]
As flexibility is key in a distributed \ac{CSE} system (cf. \cref{subsec:classifications:storage:req}) and the storage solution in general communicates via HTTP, the data aggregation service must also have a HTTP interface.
\item [Search]
The aggregation service shall have some form of search functionality.
At the very least, this feature should include a keyword search; more elaborate search types such as full-text or fuzzy search are useful but not mandatory.
\item [Complex Search via DSL]
When search queries become more complicated, it would be advantageous to have some form of \ac{DSL} which allows for combination of complex search queries.
\item [Handle Arbitrary Data]
The solution shall be able to handle any type of user feedback data.
This eliminates some services which focus solely on monitoring and logging, such as Graphite\footnote{\url{https://graphiteapp.org/}}, Graylog\footnote{\url{https://www.graylog.org/}}, Splunk\footnote{\url{https://www.splunk.com/}}, and Prometeus \footnote{\url{https://prometheus.io/}}.
\item [Aggregation Features]
In order to combine data by date and time, the solution shall have the ability to aggregate data over time.
For this specific use case of collecting and evaluating passive user feedback, I expect such a feature to be useful for evaluating an experiment that ran for a specific time frame or for evaluating the behavior of users at a certain time of day, e.g. at night.
It shall also be possible to aggregate data by location if such an information is contained in the data set.
This could be useful, for example, if an experiment with regional or cultural focus is executed, or regional differences in a global experiment are expected.
Aggregators shall be combinable, such that more complex aggregations become possible.
An example for this would be the aggregation of data over time and location.
\item [Free License]
Just as for the storage solution (cf. \cref{subsec:classifications:storage:req}), the aggregation service shall also be available in some form of free or even open source license.
\item [Performance and Scalability]
These are certainly important metrics, but hard to quantify.
As performance is not the main focus of the system that is being designed, I do not impose some sort of, either way arguable, restriction or requirement here.
Instead, I explicitly note if one of the solutions distinctly out- or underperforms in terms of performance or scalability.
\end{description}

\subsection{Candidates}

\begin{enumerate}
\item Apache Solr
\item Elasticsearch
\item Apache Hadoop
\item Apache Spark
\item Custom implementation using native Event Store aggregators
\end{enumerate}

\subsection{Evaluation}


The evaluation of the candidates introduced above yielded that Elasticsearch is the preferred data aggregation solution.
When judged in isolation of the rest of the proposed stack, Solr would also be suitable solution, but it falls behind as none of the analysis applications (cf. \cref{sec:classifications:analysis}) have Solr support.
An overview of the solutions and their support for the presented requirements is given in \cref{table:classifications:aggregation}.

Apache Solr\footnote{\url{http://lucene.apache.org/solr/}} and Elasticsearch\footnote{\url{https://www.elastic.co/guide/en/elasticsearch/}} are both built upon the Lucene\footnote{\url{http://lucene.apache.org/}} project, which is a Java-based library offering, amongst others, searching and indexing functionality.
For this reason, they are both very similar in terms of functionality and performance; both products implement their own solutions for scaling horizontally and vertically though.
They also both offer an HTTP interface which allows for submission of search queries.
Although both products are similar and meet (most of) the requirements, Elasticsearch stands out because of two points. 
First of all, Elasticsearch comes with its own \ac{DSL} for more complex queries, which can then be sent to the HTTP interface, while Solr requires using its Java API for that.
While this is a perfectly valid concept in general, for this use case the HTTP variant is the better fit.
Secondly, the two preferred analysis solutions (Kibana and Grafana, cf. \cref{sec:classifications:analysis}), both come with dedicated support for Elasticsearch, but \emph{not} for Solr.

\todo{Import of EStore data --> Logstash}

Spark\footnote{\url{http://spark.apache.org/}} is another tool by Apache, with a focus on large scale data processing.
In contrast to Elasticsearch and Solr, Spark does not come with a HTTP interface for issuing queries; instead, searches and aggregations are implemented in Java, Scala, Python, or R.
This allows for very efficient, complex and powerful data processing, but requires a lot more custom implementation when compared to other candidates that come with a query language or \ac{DSL}, therefore restricting flexibility.
The data analysis solutions that are proposed in \cref{sec:classifications:analysis} 

Hadoop ...

Event Store comes with its own aggregation functionality, called projections.
Projections are written in JavaScript and submitted to the event store via HTTP.
Using the projections feature, it is possible to write a custom aggregation service which exposes a \ac{REST} API that exposes the projection results to the analytics application.
One drawback of this approach is that it involves considerably more custom code than the other services.
Furthermore, projections are not as powerful as the search and aggregation functionalities of Elasticsearch or Solr, for example.

\begin{table}[]
\centering
\caption{Classification of data aggregation solutions. Support for arbitrary data structures and combination of aggregations is not considered here since all solutions fulfill this requirement.}
\begin{tabular}{lllllllllllllll}
\textbf{Name} & \textbf{HTTP} & \textbf{Search} & \textbf{DSL} & \textbf{Time Aggr.} & \textbf{Loc. Aggr.} & \textbf{Scalability} & \textbf{License} \\
Solr & yes & yes & no & yes & yes & yes & Apache v2 \\
Elasticsearch & yes & yes & no & yes & yes & yes & Apache v2 \\
Hadoop & ? & ? & ? & ? & ? & ? & Apache v2 \\
Spark & no & ? & no & yes & yes & yes & Apache v2 \\
Custom & yes & yes & yes & yes & no & no & -
\end{tabular}
\label{table:classifications:aggregation}
\end{table}

\section{Data Analysis Solutions}
\label{sec:classifications:analysis}

\subsection{Requirements}

\subsection{Candidates}

\begin{enumerate}
\item Kibana
\item Grafana
\item Custom implementation
\end{enumerate}

It seems that data visualization applications and services are in general rarely released as open source or free software.
Solutions which only offer a paid plan were not considered here, such as Logmatic\footnote{\url{https://logmatic.io/}}

\subsection{Evaluation}

...

In the end, Kibana was favored for its powerful integration with Elasticsearch, which I expect will improve both the speed of development and the overall quality of the prototype system.
Kibana could easily be replaced by Grafana though, as they offer very similar functionality and Grafana comes with an Elasticsearch plugin.
