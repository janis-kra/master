% !TEX root = ../thesis.tex
%
\chapter{Fundamentals}
\label{ch:fundamentals}

[insert fluff piece about fundamentals here]

\section{Experiment-driven Software Development}
\label{sec:fundamentals:edsd}

\cite{Olsson2012}

\section{Implicit User Feedback}
\label{sec:fundamentals:implicit}

\cite{Kelly:2003:IFI:959258.959260}

Gathering implicit user feedback via eye tracking software is a valid alternative to using clickthrough data, but is not considered in this thesis for two reasons.
First, requiring an eye tracker for gathering implicit user feedback of generic web applications would be impractical.
Second, cursor movement was shown to be a good approximation of where the user's gaze is located~\cite{Huang2011}, thus it is expected that using an eye tracker would not yield significant additional in the given context anyway.

\section{Continuous experimentation surveys}
\label{sec:related:surveys}

\cite{lindgren2015software}

\cite{Bosch2012}

\cite{Gutbrod2017}

\section{Controlled Experiments}
\label{sec:fundamentals:experiments}

The earliest controlled experiment dates back to the 1700s, where the crew of a British ship suffered from scurvy, a common suffering that sailors tend to experience when exposed to a limited diet like on sea ships during that time.
As the ship was charging citrus fruits, the captain of the ship decided to do an experiment: 50\% of his sailors, the \emph{treatment} group, had limes added to their diet.
The other half of the crew did not eat any citrus fruits; these were the \emph{control} group.
Although the reasons were not known during that moment, the experiment was a success: The treatment group got better, and eventually all sailors had citrus fruits added to their diet~\cite{rossi2003evaluation,marks2000progress}.

This anecdote is an example for the simplest type of controlled experiment, in which users are randomly assigned to either the control or the treatment group.
While the treatment group is treated with the new version of the system that is being tested, the control group experiences the same system as before, without the modification.
Thus, each group contains 50\% of the user base, and the results of the control group can be used in order to control wether the treatment had any statistically significant effect~\cite{Kohavi2009}.
\todo{not enough about controlled experiments}
%The test that detects wether two variants are statistically different is called the \emph{null hypothesis}.
% more details in Kohavi2009

\section{Event sourcing}
\label{sec:fundamentals:event}

%abstract: what is event sourcing
The core premise of event sourcing is that all changes to the application state are stored as immutable events, in the order in which they occur.
In aggregation, these events represent the whole application state.
%, contrary to classical relational databases,
Event sourcing was first introduced by Greg Young in 200x~\cite{???}.
While literature in form of books or scientific papers regarding event sourcing is rather rare, several extensive blog posts by \citet{WEB:Fowler:2005} and \citet{Todo:WEB:Young:200x} handle this topic.

Inherent to event sourcing is the lack of an explicit schema, just as in NoSQL databases\cite{?}.
Instead, the schema is represented implicitly within the application itself.
This is advantageous because ...
\citet{Overeem2017} investigate the problems that arise when this implicit schema changes.
\ac{IFAS} assumes that the implicit schema is not changed after its initial launch.

\subsection{Features}

% what can you do with event sourcing: aggregation, rollback
When all changes to the application state are introduced via event objects, various capabilities arise which are not possible if only the application state is stored.
As \citet{WEB:Fowler:2005} describes, these are Complete Rebuild, Temporal Query, and Event Replay:

\begin{description}
\item [Complete Rebuild]
The application state can be discarded and then rebuilt by querying all events that occurred since the beginning.
\item [Temporal Query]
Instead of doing a complete rebuild up to the present application state, the rebuild can also be done to a given date somewhere in the past.
By doing so, the application state at the given point in time is re-created.
\item [Event Replay]
In another variant of the complete rebuild, it is also possible to change a certain past event and then replay all subsequent events, which yields a variant of the current application state reflecting the consequences introduced by the modified event.
This can be done just for testing purposes, for correcting a wrong event or for fixing errors caused by events occurring in the wrong order.
\item [Reversing Events]
If the events are designed according to [...] it is possible to undo a given event by creating its reverse version.
This is easy if the event encodes the difference between the occurring event and the application state at that point, for example for events that add or subtract a numeral value.
Reversing of events is not that straightforward if this difference approach is not used -- the events should include the previous and updated value in that case, but this is not ideal in every case.
It is important to note that event reversability can also be achieved by reverting to a given snapshot of the application state and then replaying all events \emph{except} the event that shall be undone.
\item [Delayed Application]
% or whatever it is called
\end{description}

A well known example for a system which makes use of event sourcing techniques is the version control system Git\footnote{\url{http://git-scm.com/}}.
It does complete rebuilds when a project is checked out for the first time and uses temporal queries for introducing changes when switching to an existing branch. \todo{check if this is really how Git does this}

% what can you NOT do / disadvantages
Aside from its advantages, event sourcing is not an ideal storage solution for all use cases.
First of all, event sourcing introduces a level of indirection to the application, which makes the system architecture more complicated.
Additionally, as this is an altogether different concept than storing the state directly in a database, it is thus hard to grasp and thoroughly understand for most developers.
% External Updates
% External Queries

% What about code changes?

% how some of these disadvantages can be worked around (short)
As events are immutable, event sourcing does not have a delete operation, because immutability prohibits deletion.
It is however possible to create explicit removal events -- for example a \texttt{CustomerRemoved} event for deleting a customer -- which in the end results in an application state where the item in question is not present anymore.

\cite{WEB:Fowler:2011}


\subsection{Subscriptions}

Subscriptions are a mechanic that allow clients to be notified when new events are written to a stream.
There are three types of subscriptions:

\begin{description}
\item[Volatile Subscription]
The time at which the subscription is enabled is considered as its starting point.
All events that are written \emph{after} the starting point are considered by the subscription; prior events are ignored.
\item[Catch-Up Subscription]
This subscriptions allows the starting point to be supplied as a parameter by the client, e.g. in form of an event number.
Events that were written after the given starting point are served immediately to the client.
The catch-up subscription behaves just as the volatile subscription for all subsequently written events.
\item[Persistent Subscription] 
This is a special kind of catch-up subscription.
The persistent subscription stores the subscription state on the server side and supports the \emph{competing consumers} messaging pattern\footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers}}.
This pattern stipulates multiple consumers which asynchronously and independently handle messages generated by the server.
It is not relevant which of the services handle a specific message, as each behaves identically, and every message is guaranteed to be delivered \emph{at least once}.
Therefore, it is possible to set up multiple worker services for feeding event store data into a consuming service and thus enable horizontal scalability.
\end{description}

\section{Evaluation Fundamentals}
\label{sec:fundamentals:evaluation}

ISTQB fundamental testing process~\cite{graham2008foundations}
Dwell Time(?)~\cite{Liu2010}\cite{Kim2014}

\section{Docker}
\label{sec:fundamentals:docker}

Docker\footnote{\url{https://www.docker.com}} is a container platform that allows for lightweight and flexible virtualization of services and applications.
The central concept of Docker are its containers, which package application code as well as dependent libraries, binaries, and settings.
It is important to note that a Docker container does not contain a host operating system -- a main difference between a container and a \ac{VM}.
This makes containers more lightweight than \ac{VM}s, resulting in a smaller footprint in terms of CPU usage, main memory and storage.
A container is executed by the Docker engine, which is available for all modern operating systems such as Linux, Windows and MacOS.

Docker containers are defined by a \emph{Dockerfile}, which hosts a set of instructions that tell the Docker engine how to build the image which in the end is instantiated in a container (\cref{code:fundamentals:docker:dockerfile} shows an exemplary Dockerfile).
Each instruction of the Dockerfile creates another filesystem layer, which in combination produce the final image.
In this case, the Dockerfile states that the base image shall be the \texttt{nginx} image with version \texttt{1.13}; this creates the first image layer.
The \texttt{COPY} instruction copies the contents of the folder where the Dockerfile resides into the stated destination in the target filesystem, such that the copied files can be served by the NGINX instance; this creates the second image layer.
It is very important to note that the Docker engine saves image layers as diffs, meaning that a layer is described by its  difference with the layer below.
This results in high reusability of image layers: When the \texttt{COPY} instruction in the example Dockerfile is changed, only the second image layer has to be rebuilt, allowing for much faster build times.
%This suffices to bootstrap the containers itself, but in most cases additional configuration files are loaded into the container via volumes.

\lstinputlisting[label={code:fundamentals:docker:dockerfile},caption={Example Dockerfile}]{sourcecode/Dockerfile.txt}

Containers can be created and run via the Docker \ac{CLI}.
For example, the NGINX image previously described in \cref{code:fundamentals:docker:dockerfile} would first be containerized and then executed via the instructions given in \cref{code:fundamentals:docker:cli}.
The \texttt{-p} option specifies that the container's port 80 shall be exposed as port 8000 on the host operating system.

\lstinputlisting[label={code:fundamentals:docker:cli},caption={Build and run the NGINX example container via the Docker CLI},language=bash]{sourcecode/docker-cli.sh}

However, building and running images and containers via the Docker \ac{CLI} becomes tedious when a combination of multiple containers has to be managed.
\emph{Docker Compose} facilitates container composition by storing these build and run definitions in a single configuration file, using the data serialization language \ac{YAML}.
\Cref{code:fundamentals:docker:compose-yml} shows an exemplary \texttt{docker-compose.yml} file, which specifies that  the previously shown Dockerfile shall be used for building the image as a service called \texttt{server}, and that again port 8000 shall be exposed.
The image is then built, and the container subsequently run, via two commands as shown in \cref{code:fundamentals:docker:compose-run}.
While the approach using the Docker CLI may seem more concise in this limited example, Compose's advantages when combining multiple interconnected services become apparent when doing more complex container composition (cf. \cref{sec:implementation:orchestration}).

\lstinputlisting[label={code:fundamentals:docker:compose-yml},caption={Example docker-compose.yml}]{sourcecode/docker-compose-example.yml}
\lstinputlisting[label={code:fundamentals:docker:compose-run},caption={Build and run the NGINX example container via Compose},language=bash]{sourcecode/run-via-compose.sh}

The container architecture of Docker solves two challenges when creating distributed systems: application independence and bridging the development-operations gap.
Application independence is achieved by running every application in its own container which can only interact with other containers via explicitly defined interfaces (e.g. HTTP).
Secondly, Docker facilitates bridging the gap between the developers and operations teams, as the environment that the application is executed in -- the Docker container -- does not change between the development and production stages.
This makes Docker a popular technology for implementing DevOps.

Instead of Docker, a lot of alternative technologies could have been chosen as the virtualization solution.
These alternatives include services such as Vagrant, Kubernetes and Apache Mesos, some of which make use of Docker internally.
Docker was favored over the alternatives because of its maturity, good documentation, and my previous experiences with it, making this a partly subjective decision.
It would also be possible to run the passive user feedback system natively on a server or multiple servers, but this would have introduced unnecessary complications.