% !TEX root = ../thesis.tex
%
\chapter{Fundamentals}
\label{sec:fundamentals}

[insert fluff piece about fundamentals here]

\section{Experiment-driven Software Development}
\label{sec:fundamentals:edsd}

\cite{Olsson2012}

\section{Continuous experimentation surveys}
\label{sec:related:surveys}

\cite{lindgren2015software}

\cite{Bosch2012}

\cite{Gutbrod2017}

\section{Controlled Experiments}
\label{sec:fundamentals:experiments}

The earliest controlled experiment dates back to the 1700s, where the crew of a British ship suffered from scurvy, a common suffering that sailors tend to experience when exposed to a limited diet like on sea ships during that time.
As the ship was charging citrus fruits, the captain of the ship decided to do an experiment: 50\% of his sailors, the \emph{treatment} group, had limes added to their diet.
The other half of the crew did not eat any citrus fruits; these were the \emph{control} group.
Although the reasons were not known during that moment, the experiment was a success: The treatment group got better, and eventually all sailors had citrus fruits added to their diet~\cite{rossi2003evaluation,marks2000progress}.

This anecdote is an example for the simplest type of controlled experiment, in which users are randomly assigned to either the control or the treatment group.
While the treatment group is treated with the new version of the system that is being tested, the control group experiences the same system as before, without the modification.
Thus, each group contains 50\% of the user base, and the results of the control group can be used in order to control wether the treatment had any statistically significant effect~\cite{Kohavi2009}.
\todo{enough about controlled experiments?}
%The test that detects wether two variants are statistically different is called the \emph{null hypothesis}.

\section{Event sourcing \& CQRS}
\label{sec:fundamentals:event}

Event sourcing was first introduced by Greg Young in 200x~\cite{source/link}.
The core premise is that, contrary to classical relational databases, all data is stored in the form of immutable events, which represent the whole application state when aggregated.


\cite{WEB:Fowler:2005}

\cite{WEB:Fowler:2011}

\subsection{Subscriptions}

Subscriptions are a mechanic that allow clients to be notified when new events are written to a stream.
There are three types of subscriptions:

\begin{description}
\item[Volatile Subscription]
The time at which the subscription is enabled is considered as its starting point.
All events that are written \emph{after} the starting point are considered by the subscription; prior events are ignored.
\item[Catch-Up Subscription]
This subscriptions allows the starting point to be supplied as a parameter by the client, e.g. in form of an event number.
Events that were written after the given starting point are served immediately to the client.
The catch-up subscription behaves just as the volatile subscription for all subsequently written events.
\item[Persistent Subscription] 
This is a special kind of catch-up subscription.
The persistent subscription stores the subscription state on the server side and supports the \emph{competing consumers} messaging pattern\footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers}}.
This pattern stipulates multiple consumers which asynchronously and independently handle messages generated by the server.
It is not relevant which of the services handle a specific message, as each behaves identically, and every message is guaranteed to be delivered \emph{at least once}.
Therefore, it is possible to set up multiple worker services for feeding event store data into a consuming service and thus enable horizontal scalability.
\end{description}


\section{CQRS}
\label{sec:fundamentals:CQRS}

\section{Evaluation Fundamentals}
\label{sec:fundamentals:evaluation}

ISTQB fundamental testing process~\cite{graham2008foundations}

\section{Docker}
\label{sec:fundamentals:docker}

Docker\footnote{\url{https://www.docker.com}} is a container platform that allows for lightweight and flexible virtualization of services and applications.
The central concept of Docker are its containers, which package application code as well as dependent libraries, binaries, and settings.
It is important to note that a Docker container does not contain a host operating system -- a main difference between a container and a \ac{VM}.
This makes containers more lightweight than \ac{VM}s, resulting in a smaller footprint in terms of CPU usage, main memory and storage.
A container is executed by the Docker engine, which is available for all modern operating systems such as Linux, Windows and MacOS.

Docker containers are defined by a \emph{Dockerfile}, which hosts a set of instructions that tell the Docker engine how to build the image which in the end is instantiated in a container (\cref{code:fundamentals:docker:dockerfile} shows an exemplary Dockerfile).
Each instruction of the Dockerfile creates another filesystem layer, which in combination produce the final image.
In this case, the Dockerfile states that the base image shall be the \texttt{nginx} image with version \texttt{1.13}; this creates the first image layer.
The \texttt{COPY} instruction copies the contents of the folder where the Dockerfile resides into the stated destination in the target filesystem, such that the copied files can be served by the NGINX instance; this creates the second image layer.
It is very important to note that the Docker engine saves image layers as diffs, meaning that a layer is described by its  difference with the layer below.
This results in high reusability of image layers: When the \texttt{COPY} instruction in the example Dockerfile is changed, only the second image layer has to be rebuilt, allowing for much faster build times.
%This suffices to bootstrap the containers itself, but in most cases additional configuration files are loaded into the container via volumes.

\lstinputlisting[label={code:fundamentals:docker:dockerfile},caption={Example Dockerfile}]{sourcecode/Dockerfile}

Containers can be created and run via the Docker \ac{CLI}.
For example, the NGINX image previously described in \cref{code:fundamentals:docker:dockerfile} would first be containerized and then executed via the instructions given in \cref{code:fundamentals:docker:cli}.
The \texttt{-p} option specifies that the container's port 80 shall be exposed as port 8000 on the host operating system.

\lstinputlisting[label={code:fundamentals:docker:cli},caption={Build and run the NGINX example container via the Docker CLI},language=bash]{sourcecode/docker-cli.sh}

However, building and running images and containers via the Docker \ac{CLI} becomes tedious when a combination of multiple containers has to be managed.
\emph{Docker Compose} facilitates container composition by storing these build and run definitions in a single configuration file, using the data serialization language \ac{YAML}.
\Cref{code:fundamentals:docker:compose-yml} shows an exemplary \texttt{docker-compose.yml} file, which specifies that  the previously shown Dockerfile shall be used for building the image as a service called \texttt{server}, and that again port 8000 shall be exposed.
The image is then built, and the container subsequently run, via two commands as shown in \cref{code:fundamentals:docker:compose-run}.
While the approach using the Docker CLI may seem more concise in this limited example, Compose's advantages when combining multiple interconnected services become apparent when doing more complex container composition (cf. \cref{sec:implementation:orchestration}).

\lstinputlisting[label={code:fundamentals:docker:compose-yml},caption={Example docker-compose.yml}]{sourcecode/docker-compose.yml}
\lstinputlisting[label={code:fundamentals:docker:compose-run},caption={Build and run the NGINX example container via Compose},language=bash]{sourcecode/run-via-compose.sh}

The container architecture of Docker solves two challenges when creating distributed systems: application independence and bridging the development-operations gap.
Application independence is achieved by running every application in its own container which can only interact with other containers via explicitly defined interfaces (e.g. HTTP).
Secondly, Docker facilitates bridging the gap between the developers and operations teams, as the environment that the application is executed in -- the Docker container -- does not change between the development and production stages.
This makes Docker a popular technology for implementing DevOps.

Instead of Docker, a lot of alternative technologies could have been chosen as the virtualization solution.
These alternatives include services such as Vagrant, Kubernetes and Apache Mesos, some of which make use of Docker internally.
Docker was favored over the alternatives because of its maturity, good documentation, and my previous experiences with it, making this a partly subjective decision.
It would also be possible to run the passive user feedback system natively on a server or multiple servers, but this would have introduced unnecessary complications.