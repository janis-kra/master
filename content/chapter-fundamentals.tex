% !TEX root = ../thesis.tex
%
\chapter{Fundamentals}
\label{ch:fundamentals}

[insert fluff piece about fundamentals here]

\section{\acf{CSE}}
\label{sec:fundamentals:edsd}

% kopiert aus Motivation, muss noch eingearbeitet werden:
The last step in the Stairway to Heaven of \ac{CSE} is introducing an innovation system, also referred to as an \emph{experiment system}~\cite{Olsson2012}.
Several companies have published their findings about experimentation in the software development process, in the form of scientific papers or blog posts.
Amongst others, Microsoft~\cite{Kohavi2013}, LinkedIn~\cite{Xu2015}, Google~\cite{Tang2010}, Facebook~\cite{Bakshy2014}, and Netflix~\cite{WEB:Netflix:2016} issued reports about best practices and pitfalls that they experienced, all concluding that introducing experimentation into the software development process benefits both the choice about which features to implement as well as their quality.
In addition, several authors have published scientific papers in which they propose models and architectures for experimental software development~\cite{Fagerholm2014,Fagerholm2017,Johanssen2017,Lindgren2015}.
Although practitioners in the software industry often claim that they embrace experimentation and innovation in their research and development process, these practices are still not fully adopted according to a study by \citet{lindgren2015software}.
Especially the systematic and continuous aspects of experimentation lack adoption.

\cite{Olsson2012}
\cite{Bosch2012}
 what \citet{Bosch2012} calls \emph{data-based decision making}.

The results of such an experiment are then used for deciding wether the feature should become a part of the final product.
Thus, the goal in this case is to increase the effectiveness of developing a software product by implementing the \emph{right} features.
Experiments are also used in order to compare different implementations of the same feature, thus improving the quality of an existing feature.
This approach is contrary to the more classical software development methods, in which the stakeholders make certain assumptions which result in the envisioning of a feature, with feedback by a customer or stakeholder being given only much later in the development process~\cite{Bosch2012}.

The core premise in innovation experiment systems is the continuous implementation and validation of assumptions about envisioned features via experiments in short sprints~\cite{Bosch2014}.
The results of such an experiment are then used for deciding wether the feature should become a part of the final product.
Thus, the goal in this case is to increase the effectiveness of developing a software product by implementing the \emph{right} features.
Experiments are also used in order to compare different implementations of the same feature, thus improving the quality of an existing feature.
This approach is contrary to the more classical software development methods, in which the stakeholders make certain assumptions which result in the envisioning of a feature, with feedback by a customer or stakeholder being given only much later in the development process~\cite{Bosch2012}.

\cite{Bosch2012}: First, it is focused on continuously evolving the software by frequently deploying new versions. Second, customers and customer usage data play a central role throughout the development process. Third, development is focused on innovation and testing as many ideas as possible with customers to drive customer satisfaction and, consequently, revenue growth.


\section{Evolutionary Software Architecture}
\label{sec:fundamentals:evolutionary}

\citet{ford2017building} describe that evolutionary architectures try to battle the common wisdom that a software architecture is in general difficult to change once it is built.
The underlying assumption is that it is hard to anticipate all needs of the architecture beforehand and thus, changes become expensive in regards to both money and time.
By factoring change into the architecture itself, incremental changes are claimed to be possible later in the development process, when a conventional architecture would not have allowed for such a change with as little effort.
This can for example be achieved by using a microservice architecture~\cite{WEB:EvolArch:2016}.

...

\section{Passive User Feedback}
\label{sec:fundamentals:implicit}

\cite{Kelly:2003:IFI:959258.959260}

Gathering passive user feedback via eye tracking software is a valid alternative to using clickthrough data, but is not considered in this thesis for two reasons.
First, requiring an eye tracker for gathering implicit user feedback of generic web applications would be impractical.
Second, cursor movement was shown to be a good approximation of where the user's gaze is located~\cite{Huang2011}, thus it is expected that using an eye tracker would not yield significant additional in the given context anyway.

\section{Continuous experimentation surveys}
\label{sec:related:surveys}

\cite{lindgren2015software}

\cite{Bosch2012}

\cite{Gutbrod2017}

\section{Controlled Experiments}
\label{sec:fundamentals:experiments}

The earliest controlled experiment dates back to the 1700s, where the crew of a British ship suffered from scurvy, a common suffering that sailors tend to experience when exposed to a limited diet like on sea ships during that time.
As the ship was charging citrus fruits, the captain of the ship decided to do an experiment: 50\% of his sailors, the \emph{treatment} group, had limes added to their diet.
The other half of the crew did not eat any citrus fruits; these were the \emph{control} group.
Although the reasons were not known during that moment, the experiment was a success: The treatment group got better, and eventually all sailors had citrus fruits added to their diet~\cite{rossi2003evaluation,marks2000progress}.

This anecdote is an example for the simplest type of controlled experiment, in which users are randomly assigned to either the control or the treatment group.
While the treatment group is treated with the new version of the system that is being tested, the control group experiences the same system as before, without the modification.
Thus, each group contains 50\% of the user base, and the results of the control group can be used in order to control wether the treatment had any statistically significant effect~\cite{Kohavi2009}.
\todo{not enough about controlled experiments}
%The test that detects wether two variants are statistically different is called the \emph{null hypothesis}.
% more details in Kohavi2009

...

When executing controlled experiments, an important factor is the choice of metric that is used for comparign results.
\citet{Kohavi2013a} stresses the importance of defining an \ac{OEC} which distills important metrics such as revenue or customer satisfaction in one single metric.
Having a fitting \ac{OEC} eliminates the need to carefully balance out the other metrics, as they are represented by the \ac{OEC}.
\citeauthor{Kohavi2013a} gives an example where market share is a bad \ac{OEC} for a search engine, as worsening the search algorithm would increase the amount of search queries in the short term and thus increase market share; in the long run, users would use an alternative product and thus this \ac{OEC} would drop again.
Sessions per user would be a much better \ac{OEC} in this example, \citeauthor{Kohavi2013a} says.


\section{Event sourcing}
\label{sec:fundamentals:event}

%abstract: what is event sourcing
The core premise of event sourcing~\cite{WEB:Fowler:2005} is that all changes to the application state are stored as immutable events, in the order in which they occur.
In aggregation, these events represent the whole application state.
%, contrary to classical relational databases,
\citet{Overeem2017} and \citet{Erb2016} have done some research on event sourcing and \citet{evans2004domain} grazes the topic in his book, stating that event sourcing is useful for X.
Apart from these few examples, literature in form of books or scientific papers regarding event sourcing is rather rare.
Several extensive blog posts by \citet{WEB:Fowler:2005} and \citet{young2010whyeventsourcing} explain the concepts and methods in great extent though.
\todo{vertiefen}

Inherent to event sourcing is the lack of an explicit schema, just as in NoSQL databases~\cite{fowler2013schemaless}.
Instead, the schema is represented implicitly within the application itself.
This enables more flexibility, as any type of data can be saved in storage without defining a schema first.
However, it also comes with some drawbacks, especially when this implicit schema changes; \citet{Overeem2017} investigate these.
\ac{IFAS} assumes that the implicit schema is not changed after its initial launch.

\subsection{Features}

% what can you do with event sourcing: aggregation, rollback
When all changes to the application state are introduced via event objects, various capabilities arise which are not possible if only the application state is stored.
As \citet{WEB:Fowler:2005} describes, these are Complete Rebuild, Temporal Query, and Event Replay:

\begin{description}
\item [Complete Rebuild]
The application state can be discarded and then rebuilt by querying all events that occurred since the beginning.
\item [Temporal Query]
Instead of doing a complete rebuild up to the present application state, the rebuild can also be done to a given date somewhere in the past.
By doing so, the application state at the given point in time is re-created.
\item [Event Replay]
In another variant of the complete rebuild, it is also possible to change a certain past event and then replay all subsequent events, which yields a variant of the current application state reflecting the consequences introduced by the modified event.
This can be done just for testing purposes, for correcting a wrong event or for fixing errors caused by events occurring in the wrong order.
\item [Reversing Events]
If the events are designed according to [...] it is possible to undo a given event by creating its reverse version.
This is easy if the event encodes the difference between the occurring event and the application state at that point, for example for events that add or subtract a numeral value.
Reversing of events is not that straightforward if this difference approach is not used -- the events should include the previous and updated value in that case, but this is not ideal in every case.
It is important to note that event reversability can also be achieved by reverting to a given snapshot of the application state and then replaying all events \emph{except} the event that shall be undone.
\item [Delayed Application]
% or whatever it is called
\item [Parallel Model]
\end{description}

A well known example for a system which makes use of event sourcing techniques is the version control system Git\footnote{\url{http://git-scm.com/}}.
It does complete rebuilds when a project is checked out for the first time and uses temporal queries for introducing changes when switching to an existing branch. \todo{check if this is really how Git does this}

% what can you NOT do / disadvantages
Aside from its advantages, event sourcing is not an ideal storage solution for all use cases.
First of all, event sourcing introduces a level of indirection to the application, which makes the system architecture more complicated.
Additionally, as this is an altogether different concept than storing the state directly in a database, it is thus hard to grasp and thoroughly understand for most developers.
% External Updates
% External Queries

% What about code changes?

% how some of these disadvantages can be worked around (short)
As events are immutable, event sourcing does not have a delete operation, because immutability prohibits deletion.
It is however possible to create explicit removal events -- for example a \texttt{CustomerRemoved} event for deleting a customer -- which in the end results in an application state where the item in question is not present anymore.

\cite{WEB:Fowler:2011}


\subsection{Subscriptions}

Subscriptions are a mechanic that allow clients to be notified when new events are written to a stream.
There are three types of subscriptions:

\begin{description}
\item[Volatile Subscription]
The time at which the subscription is enabled is considered as its starting point.
All events that are written \emph{after} the starting point are considered by the subscription; prior events are ignored.
\item[Catch-Up Subscription]
This subscriptions allows the starting point to be supplied as a parameter by the client, e.g. in form of an event number.
Events that were written after the given starting point are served immediately to the client.
The catch-up subscription behaves just as the volatile subscription for all subsequently written events.
\item[Persistent Subscription] 
This is a special kind of catch-up subscription.
The persistent subscription stores the subscription state on the server side and supports the \emph{competing consumers} messaging pattern\footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/patterns/competing-consumers}}.
This pattern stipulates multiple consumers which asynchronously and independently handle messages generated by the server.
It is not relevant which of the services handle a specific message, as each behaves identically, and every message is guaranteed to be delivered \emph{at least once}.
Therefore, it is possible to set up multiple worker services for feeding event store data into a consuming service and thus enable horizontal scalability.
\end{description}

\section{Evaluation Fundamentals}
\label{sec:fundamentals:evaluation}

ISTQB fundamental testing process~\cite{graham2008foundations}
Dwell Time(?)~\cite{Liu2010}\cite{Kim2014}
Experiment validity

\citet{Easterbrook2008a} list four factors for validity of empirical work in general: Construct validity, internal validity, external validity, and reliability.


\section{Docker}
\label{sec:fundamentals:docker}

Docker\footnote{\url{https://www.docker.com}} is a container platform that allows for lightweight and flexible virtualization of services and applications.
The central concept of Docker are its containers, which package application code as well as dependent libraries, binaries, and settings.
It is important to note that a Docker container does not contain a host operating system -- a main difference between a container and a \ac{VM}.
This makes containers more lightweight than \ac{VM}s, resulting in a smaller footprint in terms of CPU usage, main memory and storage.
A container is executed by the Docker engine, which is available for all modern operating systems such as Linux, Windows and MacOS.

Docker containers are defined by a \emph{Dockerfile}, which hosts a set of instructions that tell the Docker engine how to build the image which in the end is instantiated in a container (\cref{code:fundamentals:docker:dockerfile} shows an exemplary Dockerfile).
Each instruction of the Dockerfile creates another filesystem layer, which in combination produce the final image.
In this case, the Dockerfile states that the base image shall be the \texttt{nginx} image with version \texttt{1.13}; this creates the first image layer.
The \texttt{COPY} instruction copies the contents of the folder where the Dockerfile resides into the stated destination in the target filesystem, such that the copied files can be served by the NGINX instance; this creates the second image layer.
It is very important to note that the Docker engine saves image layers as diffs, meaning that a layer is described by its  difference with the layer below.
This results in high reusability of image layers: When the \texttt{COPY} instruction in the example Dockerfile is changed, only the second image layer has to be rebuilt, allowing for much faster build times.
%This suffices to bootstrap the containers itself, but in most cases additional configuration files are loaded into the container via volumes.

\lstinputlisting[label={code:fundamentals:docker:dockerfile},caption={Example Dockerfile}]{sourcecode/Dockerfile.txt}

Containers can be created and run via the Docker \ac{CLI}.
For example, the NGINX image previously described in \cref{code:fundamentals:docker:dockerfile} would first be containerized and then executed via the instructions given in \cref{code:fundamentals:docker:cli}.
The \texttt{-p} option specifies that the container's port 80 shall be exposed as port 8000 on the host operating system.

\lstinputlisting[label={code:fundamentals:docker:cli},caption={Build and run the NGINX example container via the Docker CLI},language=bash]{sourcecode/docker-cli.sh}

However, building and running images and containers via the Docker \ac{CLI} becomes tedious when a combination of multiple containers has to be managed.
\emph{Docker Compose} facilitates container composition by storing these build and run definitions in a single configuration file, using the data serialization language \ac{YAML}.
\Cref{code:fundamentals:docker:compose-yml} shows an exemplary \texttt{docker-compose.yml} file, which specifies that  the previously shown Dockerfile shall be used for building the image as a service called \texttt{server}, and that again port 8000 shall be exposed.
The image is then built, and the container subsequently run, via two commands as shown in \cref{code:fundamentals:docker:compose-run}.
While the approach using the Docker CLI may seem more concise in this limited example, Compose's advantages when combining multiple interconnected services become apparent when doing more complex container composition (cf. \cref{sec:implementation:orchestration}).

\lstinputlisting[label={code:fundamentals:docker:compose-yml},caption={Example docker-compose.yml}]{sourcecode/docker-compose-example.yml}
\lstinputlisting[label={code:fundamentals:docker:compose-run},caption={Build and run the NGINX example container via Compose},language=bash]{sourcecode/run-via-compose.sh}

The container architecture of Docker solves two challenges when creating distributed systems: application independence and bridging the development-operations gap.
Application independence is achieved by running every application in its own container which can only interact with other containers via explicitly defined interfaces (e.g. HTTP).
Secondly, Docker facilitates bridging the gap between the developers and operations teams, as the environment that the application is executed in -- the Docker container -- does not change between the development and production stages.
This makes Docker a popular technology for implementing DevOps.

Instead of Docker, a lot of alternative technologies could have been chosen as the virtualization solution.
These alternatives include services such as Vagrant, Kubernetes and Apache Mesos, some of which make use of Docker internally.
Docker was favored over the alternatives because of its maturity, good documentation, and my previous experiences with it, making this a partly subjective decision.
It would also be possible to run the passive user feedback system natively on a server or multiple servers, but this would have introduced unnecessary complications.