% !TEX root = ../proposal.tex
%
\chapter{Approach}
\label{sec:approach}

In order to achieve the objectives described in \cref{sec:objectives}, the working time of the thesis will roughly be divided into three segments: Research, implementation and evaluation.
Putting the gained findings into writing is part of each of these segments.
A detailed work plan can be found in \cref{sec:workplan}.

A huge part of the research phase will be dedicated to reading up on the central topics for this thesis, namely but not exclusively event sourcing, \ac{CQRS}, innovation experiment systems, passive user feedback and controlled experiments.
Aside from that, some decisions regarding the architecture and technology require additional research.
This involves researching, comparing and finally choosing an appropriate storage solution.
Current findings suggest that an event store, and in particular the Event Store reference implementation\footnote{\url{https://eventstore.org/}}, is a good fit for this.
The advantages for using event sourcing in this context are as follows:

\begin{enumerate}
\item Event replayability could be used to further mitigate the risk of experimentation: If an experimental change in the software causes some faulty event to be created, it can later be modified or deleted.
Contrary to classical relational databases, the overall application state in an event store would not be corrupted by this.
It would also be possible to create separate event stores just for the experiment, which could later be applied to the main event store by replaying the events from the experimental store.
\item When an experiment has to be analysed, temporal queries could be used for retrieving all events in the experiments time frame.
\item Event sourcing and \ac{CQRS} advocate a distributed system architecture -- which is also true for innovation experiment systems~\cite{ford2017building}.
\end{enumerate}

In particular, the temporal query feature could render additional aggregation services obsolete.
Additional backup systems that roll back changes issued by an experiment could also possibly be removed because of an event stores inherent ability to roll back and redo changes.
These assumptions would have to be validated over the course of the actual thesis.

A solution for aggregating and analysing the data has to be researched as well; the aforementioned temporal query feature is one candidate for this, another is Elasticsearch\footnote{\url{https://www.elastic.co/}}.
In order to achieve high platform independence and flexibility, a container platform such as Docker\footnote{\url{https://www.docker.com/}} will be used which also requires some research as to which platform to use.
In order to be able to decide wether to use the RICO dataset~\cite{Deka:2017:Rico} instead of a custom client application for the evaluation part, this dataset will have to be assessed further.

The first task in the implementation phase will be to finalise the system architecture.
While the architecture is not expected to be overly complex and a first draft already exists at the time of writing (cf. \cref{fig:system:vision}), some alterations are possible and expected.
This task also involves deciding which data to save to the storage layer and which logical structure this data has to have.
When the system architecture design is finalised, the actual implementation can begin, which first of all involves setting up and configuring the storage solution.
If user feedback data is to be generated from a client application and not using the RICO dataset, the actual feedback logging has to be implemented as well.
Depending on the choice of aggregation service, if any, this service also has to be set up and / or implemented; the same holds for the analysis solution.

Finally, the implemented solution will be evaluated by executing up to two experiments.
The first experiment for the evaluation will be a questionnaire in which usability experts answer questions about a given application.
The results of the questionnaire will be compared to the passive user feedback data of the given application.
Depending on the means of collecting this user data, an experiment for collecting this data has to be designed and executed with some test subjects; this is not necessary if the RICO dataset is chosen as the source of user data.
The experiments have to be planned and executed carefully in order to have high internal and external validity~\cite{Huitt2010}.
Existing studies about best practices and pitfalls of running controlled experiments can be useful here~\cite{Kohavi2009}.

%* Research storage solutions --> choose Event Store
%    - also includes researching\&comparing event store implementations
%* Research aggregation solutions --> Pure Event Store, Elasticsearch or some other service like ES
%* Research analysis solutions --> (probably) choose Kibana
%* Event structure - "what is logged"
%* Implementation / Configuration / Setup:
%    - Event Store + Sync to aggregation
%    - Elasticsearch
%    - Kibana
%* Evaluation Custom exp. vs. RICO
%    - Design experiment (validity *and* relevance!)
%        + validity: see notes in "Internal and external validity"
%        + relevance: see Kohavi2009
%    - Implement exp.
%    - Execute exp.
%    - Evaluate exp.
