@inproceedings{Deka:2017:Rico,
author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
booktitle = {Proceedings of the 30th Annual Symposium on User Interface Software and Technology},
file = {:Users/jk/Documents/Uni/master/refs/review/rico.pdf:pdf},
keywords = {Mobile app design; design mining; design search; a},
mendeley-groups = {Master Thesis},
series = {UIST '17},
title = {{Rico: A Mobile App Dataset for Building Data-Driven Design Applications}},
year = {2017}
}
@article{Johanssen2017,
abstract = {Continuous software engineering (CSE) employs activities such as continuous integration and continuous delivery to support software evolution. Another aspect of software evolution is knowledge management. There are two important knowledge types: usage knowledge derives from explicit and implicit user feedback and helps to understand how users utilize software. Decision knowledge encompasses decisions and their rationale on all aspects of the software lifecycle. Both knowledge types represent important information sources for developers to improve the CSE activities and the software product. We envision an integration of usage and decision knowledge in the CSE lifecycle. This extension consists of a monitoring and feedback component for user understanding as well as a knowledge repository and dashboard component for knowledge visualization and analysis. Usage and decision knowledge introduce challenges when integrating them in CSE. In this paper, we present our vision and detail the challenges.},
annote = {-explizit von Bj{\"{o}}rn vorgeschlagen
-als related work geeignet: Breiterer Rahmen und etwas theoretischer, aber Teil des Papers ist ein System zum Sammeln von impliziten User Feedback und Anreicherung des expliziten UF mit dem impliziten},
author = {Johanssen, Jan Ole and Kleebaum, Anja and Bruegge, Bernd and Paech, Barbara},
doi = {10.1109/VISSOFT.2017.18},
file = {:Users/jk/Documents/Uni/master/refs/johanssen2017.pdf:pdf},
isbn = {9781538610039},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Continuous software engineering,Decision knowledge,Usage knowledge,experimentation,fundamentals,motivation,related work},
mendeley-groups = {Master Thesis},
mendeley-tags = {experimentation,fundamentals,motivation,related work},
pages = {7--11},
title = {{Towards a systematic approach to integrate usage and decision knowledge in continuous software engineering}},
volume = {1806},
year = {2017}
}
@online{WEB:Netflix:2016,
	Author = {Urban, Steve and Sreenivasan, Rangarajan and Kannan, Vineet},
	Title = {It’s All A/Bout Testing: The Netflix Experimentation Platform},
	Url = {https://medium.com/netflix-techblog/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15},
	Urldate = {2017-11-06},
	Year = {2016}
}
@book{thomke2003experimentation,
annote = {ref. in Kohavi2009 - not sure if relevant},
author = {Thomke, Stefan H},
publisher = {Harvard Business Press},
title = {{Experimentation matters: unlocking the potential of new technologies for innovation}},
year = {2003}
}
@book{keppel1992introduction,
annote = {ref. in Kohavi2009 - not sure if relevant},
author = {Keppel, Geoffrey and Saufley, William H and Tokunaga, Howard},
publisher = {Macmillan},
title = {{Introduction to design and analysis: A student's handbook}},
year = {1992}
}
@book{box2005statistics,
annote = {ref. in Kohavi2009 - not sure if relevant},
author = {Box, George E P and Hunter, J Stuart and Hunter, William Gordon},
publisher = {Wiley-Interscience New York},
title = {{Statistics for experimenters: design, innovation, and discovery}},
volume = {2},
year = {2005}
}
@book{mason2003statistical,
annote = {ref. in Kohavi2009 - not sure if relevant},
author = {Mason, Robert L and Gunst, Richard F and Hess, James L},
publisher = {John Wiley {\&} Sons},
title = {{Statistical design and analysis of experiments: with applications to engineering and science}},
volume = {474},
year = {2003}
}
@article{mark2009quasi,
annote = {ref. in Kohavi2009 - not sure if relevant},
author = {Mark, Melvin M and Reichardt, Charles S},
journal = {The SAGE handbook of applied social research methods},
pages = {182--213},
publisher = {Sage Publications, Inc Thousand Oaks{\^{}} eCA CA},
title = {{Quasi-experimentation}},
year = {2009}
}
@book{roy2001design,
annote = {Reference / Definition for OEC},
author = {Roy, Ranjit K},
publisher = {John Wiley {\&} Sons},
title = {{Design of experiments using the Taguchi approach: 16 steps to product and process improvement}},
year = {2001}
}
@book{Wohlin2000,
abstract = {Are inspections more effective than structural testing? Are object-oriented programs easier to maintain than function-oriented programs? You may argue for or against, but you never know until you have evaluated the issues empirically, for example, through controlled experiments. This book addresses the challenges of performing empirically based evaluations in software engineering. Experimentation in Software Engineering - An Introduction provides a comprehensive introduction to experimentation in software engineering. It works a guide to setting up and conducting an experiment. It presents an experiment process, a toolbox for design and analysis of experiments and examples that can be used for training purposes. The book may be used for several different categories of readers. It acts as an introduction to experimentation for students. It supports teachers in turning their software engineering courses more empirical. For researchers, the book can be used as an introduction to empirical research methodology. Finally, practitioners can use the books as a ``cookbook'' when evaluation new methods or techniques.},
annote = {Reference used in "Guidelines for conducting and reporting case study research in software engineering" when describing experiments},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"{o}}orn and Wessl{\'{e}}n, Anders},
booktitle = {Springer Netherlands},
doi = {10.1007/978-3-642-29044-2},
eprint = {arXiv:1011.1669v3},
isbn = {0792386825},
issn = {10991689},
pages = {228},
pmid = {11790255},
title = {{Experimentation in software engineering: an introduction}},
volume = {15},
year = {2000}
}
@article{Fagerholm2017,
abstract = {Context: Development of software-intensive products and services increasingly occurs by continuously deploying product or service increments, such as new features and enhancements, to customers. Product and service developers must continuously find out what customers want by direct customer feedback and usage behaviour observation. Objective: This paper examines the preconditions for setting up an experimentation system for continuous customer experiments. It describes the RIGHT model for Continuous Experimentation (Rapid Iterative value creation Gained through High-frequency Testing), illustrating the building blocks required for such a system. Method: An initial model for continuous experimentation is analytically derived from prior work. The model is matched against empirical case study findings from two startup companies and further developed. Results: Building blocks for a continuous experimentation system and infrastructure are presented. Conclusions: A suitable experimentation system requires at least the ability to release minimum viable products or features with suitable instrumentation, design and manage experiment plans, link experiment results with a product roadmap, and manage a flexible business strategy. The main challenges are proper, rapid design of experiments, advanced instrumentation of software to collect, analyse, and store relevant data, and the integration of experiment results in both the product development cycle and the software development process.},
annote = {- model for continuous experimentation
- broader context for my experimentation architecture
- lessons learned not read yet, may be useful},
author = {Fagerholm, Fabian and {Sanchez Guinea}, Alejandro and M{\"{a}}enp{\"{a}}{\"{a}}, Hanna and M{\"{u}}nch, J{\"{u}}rgen},
doi = {10.1016/j.jss.2016.03.034},
file = {:Users/jk/Documents/Uni/master/refs/fagerholm2017.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Agile software development,Continuous experimentation,Lean software development,Product development,Software architecture,Software development process,fundamentals,related work},
mendeley-tags = {fundamentals,related work},
pages = {292--305},
title = {{The RIGHT model for Continuous Experimentation}},
volume = {123},
year = {2017}
}
@article{Gutbrod2017,
annote = {- seems to be out of scope for me, as the focus is on how startups do (cont.) experimentation
- interesting for work!},
author = {Gutbrod, Matthias},
file = {:Users/jk/Documents/Uni/master/refs/gutbrod2017.pdf:pdf},
keywords = {interesting,out-of-scope},
mendeley-tags = {interesting,out-of-scope},
number = {November},
title = {{How Do Software Startups Approach Experimentation ? Empirical Results from a Qualitative Interview Study How Do Software Startups Approach Experimentation ?}},
year = {2017}
}
@article{Huitt2010,
author = {Huitt, W. and Hummel, J. and Kaeck, D.},
doi = {10.1136/bmj.c1705},
file = {:Users/jk/Documents/Uni/master/refs/internal-vs-external-validity.pdf:pdf},
issn = {0959-8138},
journal = {Bmj},
keywords = {fundamentals},
mendeley-tags = {fundamentals},
number = {mar31 1},
pages = {c1705--c1705},
title = {{Internal and external validity}},
volume = {340},
year = {2010}
}
@book{Robson2002,
abstract = {A resource for users of social research methods in applied settings.},
annote = {Reference used in "Guidelines for conducting and reporting case study research in software engineering" when describing experiments},
author = {Robson, Colin},
booktitle = {Edition. Blackwell Publishing. Malden},
isbn = {978-1-4051-82409},
pages = {587},
title = {{Real World Research. 2nd}},
url = {http://www.dem.fmed.uc.pt/Bibliografia/Livros{\_}Educacao{\_}Medica/Livro34.pdf},
year = {2002}
}
@article{Lindgren2015,
abstract = {Context: An experiment-driven approach to software product and service development is gaining increasing attention as a way to channel limited resources to the efficient creation of customer value. In this approach, software capabilities are developed incrementally and validated in continuous experiments with stakeholders such as customers and users. The experiments provide factual feedback for guiding subsequent development. Objective: This paper explores the state of the practice of experimentation in the software industry. It also identifies the key challenges and success factors that practitioners associate with the approach. Method: A qualitative survey based on semi-structured interviews and thematic coding analysis was conducted. Ten Finnish software development companies, represented by thirteen interviewees, participated in the study. Results: The study found that although the principles of continuous experimentation resonated with industry practitioners, the state of the practice is not yet mature. In particular, experimentation is rarely systematic and continuous. Key challenges relate to changing the organizational culture, accelerating the development cycle speed, and finding the right measures for customer value and product success. Success factors include a supportive organizational culture, deep customer and domain knowledge, and the availability of the relevant skills and tools to conduct experiments. Conclusions: It is concluded that the major issues in moving towards continuous experimentation are on an organizational level; most significant technical challenges have been solved. An evolutionary approach is proposed as a way to transition towards experiment-driven development.},
annote = {-a little too meta for my needs
-seems to be a more detailed version of "Software development as an experiment system: a qualitative survey on the state of the practice"
-similar findings in the end, but more detailed explanation etc.
-read in more depth when necessary},
author = {Lindgren, Eveliina and M{\"{u}}nch, J{\"{u}}rgen},
doi = {10.1016/j.infsof.2016.04.008},
file = {:Users/jk/Documents/Uni/master/refs/lindgren2016.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Continuous experimentation,Experiment-driven Software development,Hypothesis-driven Software development,Lean UX,Lean startup,Product management,experimentation,motivation,skimmed},
mendeley-tags = {experimentation,motivation,skimmed},
pages = {1--35},
title = {{Raising the odds of success: The current state of experimentation in product development}},
year = {2015}
}
@article{Easterbrook2008,
abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
doi = {10.1007/978-1-84800-044-5_11},
eprint = {arXiv:1011.1669v3},
file = {:Users/jk/Documents/Uni/master/refs/easterbrook2008.pdf:pdf},
isbn = {978-1-84800-043-8},
issn = {{\textless}null{\textgreater}},
journal = {Guide to Advanced Empirical Software Engineering},
keywords = {out-of-scope},
mendeley-tags = {out-of-scope},
number = {October 2014},
pages = {285--311},
pmid = {6565},
title = {{Selecting Empirical Methods for Software Engineering Research}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}11},
year = {2008}
}
@incollection{Easterbrook2008a,
abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
booktitle = {Guide to Advanced Empirical Software Engineering},
doi = {10.1007/978-1-84800-044-5_11},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-84800-043-8},
issn = {{\textless}null{\textgreater}},
pages = {285--311},
pmid = {6565},
title = {{Selecting Empirical Methods for Software Engineering Research}},
url = {http://link.springer.com/10.1007/978-1-84800-044-5{\_}11},
year = {2008}
}
@article{Bakshy2014,
abstract = {Online experiments are widely used to compare specific design alternatives, but they can also be used to produce generalizable knowledge and inform strategic decision making. Doing so often requires sophisticated experimental designs, iterative refinement, and careful logging and analysis. Few tools exist that support these needs. We thus introduce a language for online field experiments called PlanOut. PlanOut separates experimental design from application code, allowing the experimenter to concisely describe experimental designs, whether common "A/B tests" and factorial designs, or more complex designs involving conditional logic or multiple experimental units. These latter designs are often useful for understanding causal mechanisms involved in user behaviors. We demonstrate how experiments from the literature can be implemented in PlanOut, and describe two large field experiments conducted on Facebook with PlanOut. For common scenarios in which experiments are run iteratively and in parallel, we introduce a namespaced management system that encourages sound experimental practice.},
annote = {Describes quite detailed how Facebook runs experiments using the PlanOut language, some of the architecture and best practices regarding experimentation workflow, analysis etc.


Could use PlanOut for defining experiments (if needed)},
archivePrefix = {arXiv},
arxivId = {1409.3174},
author = {Bakshy, Eytan and Eckles, Dean and Bernstein, Michael S.},
doi = {10.1145/2566486.2567967},
eprint = {1409.3174},
file = {:Users/jk/Documents/Uni/master/refs/bakshy2014.pdf:pdf},
isbn = {9781450327442},
issn = {1450327443},
keywords = {3,5,a,analysis,b testing,categories and subject descriptors,evaluation,experimentation,group and organization interfaces,h,implementation,interesting,methodology,online experiments,related work,skimmed,toolkits},
mendeley-tags = {analysis,experimentation,implementation,interesting,skimmed,related work},
title = {{Designing and Deploying Online Field Experiments}},
url = {http://arxiv.org/abs/1409.3174},
year = {2014}
}
@article{Kohavi2009,
abstract = {The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments, A/B tests (and their generalizations), split tests, Control/Treatment tests, MultiVariable Tests (MVT) and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person's Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.},
annote = {TODO: Read this; esp. before kohavi2007 as they seem to be closely related},
author = {Kohavi, Ron and Longbotham, Roger and Sommerfield, Dan and Henne, Randal M.},
doi = {10.1007/s10618-008-0114-1},
file = {:Users/jk/Documents/Uni/master/refs/kohavi2009.pdf:pdf},
isbn = {1384-5810},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {A/B testing,Controlled experiments,MVT,MultiVariable Testing,Website optimization,a/b testing,e-commerce,experimentation,fundamentals,implementation,introduction,motivation},
mendeley-tags = {a/b testing,experimentation,fundamentals,implementation,introduction,motivation},
number = {1},
pages = {140--181},
title = {{Controlled experiments on the web: Survey and practical guide}},
volume = {18},
year = {2009}
}
@article{Kohavi2014,
abstract = {Web site owners, from small web sites to the largest properties that include Amazon, Facebook, Google, LinkedIn, Microsoft, and Yahoo, attempt to improve their web sites, optimizing for criteria ranging from repeat usage, time on site, to revenue. Having been involved in running thousands of controlled experiments at Amazon, Booking.com, LinkedIn, and multiple Microsoft properties, we share seven rules of thumb for experimenters, which we have generalized from these experiments and their results. These are principles that we believe have broad applicability in web optimization and analytics outside of controlled experiments, yet they are not provably correct, and in some cases exceptions are known. To support these rules of thumb, we share multiple real examples, most being shared in a public paper for the first time. Some rules of thumb have previously been stated, such as 'speed matters,' but we describe the assumptions in the experimental design and share additional experiments that improved our understanding of where speed matters more: certain areas of the web page are more critical. This paper serves two goals. First, it can guide experimenters with rules of thumb that can help them optimize their sites. Second, it provides the KDD community with new research challenges on the applicability, exceptions, and extensions to these, one of the goals for KDD's industrial track.},
annote = {- useful when designing, reasoning about and evaluating the experiments
- but probably too detailed about analysis of experiments
- did not understand the formulas etc.},
author = {Kohavi, Ron and Deng, Alex and Longbotham, Roger and Xu, Ya},
doi = {10.1145/2623330.2623341},
file = {:Users/jk/Documents/Uni/master/refs/kohavi2014.pdf:pdf},
isbn = {9781450329569},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
keywords = {a,b testing,booking,com,controlled experiments,evaluation,experimentation,having worked,including amazon,on optimizing different sites,online experiments,our experience in optimizing,web sites comes from},
mendeley-tags = {evaluation,experimentation},
pages = {1857--1866},
title = {{Seven rules of thumb for web site experimenters}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623341},
year = {2014}
}
@article{Kohavi2007,
abstract = {The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments (single-factor or factorial designs), A/B tests (and their generalizations), split tests, Control/Treatment tests, and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on- investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person's Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.},
annote = {kohavi2009 is a more recent version of this paper.},
author = {Kohavi, Ron and Henne, Randal M. Rm and Sommerfield, Dan},
doi = {10.1145/1281192.1281295},
file = {:Users/jk/Documents/Uni/master/refs/kohavi2007pdf.pdf:pdf},
isbn = {9781595936097},
issn = {1595936092},
journal = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '07},
keywords = {A/B testing,Controlled experiments,e-commerce,out-of-scope},
mendeley-tags = {out-of-scope},
pages = {959--967},
title = {{Practical guide to controlled experiments on the web}},
url = {http://dl.acm.org/citation.cfm?id=1281295{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1281192.1281295{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1281295},
volume = {2007},
year = {2007}
}
@article{Kohavi2010,
abstract = {Tracking users' online clicks and form submits (e.g., searches) is critical for web analytics, controlled experiments, and business intelligence. Most sites use web beacons to track user actions, but waiting for the beacon to return on clicks and submits slows the next action (e.g., showing search results or the destination page). One possibility is to use a short timeout and common wisdom is that the more time given to the tracking mechanism (suspending the user action), the lower the data loss. Research from Amazon, Google, and Microsoft showed that small delays of a few hundreds of milliseconds have dramatic negative impact on revenue and user experience (Kohavi, et al., 2009 p. 173), yet we found that many websites allow long delays in order to collect click. For example, until March 2010, multiple Microsoft sites waited for click beacons to return with a 2-second timeout, introducing a delay of about 400msec on user clicks. To the best of our knowledge, this is the first published empirical study of the subject under a controlled environment. While we confirm the common wisdom about the tradeoff in general, a surprising result is that the tradeoff does not exist for the most common browser family, Microsoft Internet Explorer (IE), where no delay suffices. This finding has significant implications for tracking users since no waits is required to prevent data loss for IE browsers and it could significantly improve revenue and user experience. The recommendations here have been implemented by the MSN US home page and Hotmail.},
annote = {-very technical paper about implementation details of user tracking
-could use this to argue why it is nice to have user tracking out-of-the-box via event-sourcing
-should also use a more recent paper as a reference though},
author = {Kohavi, Ron and Messner, David and Eliot, Seth and Ferres, Juan Lavista and Henne, Randy and Kannappan, Vignesh and Wang, Justin},
file = {:Users/jk/Documents/Uni/master/refs/kohavi2010pdf.pdf:pdf},
journal = {Search},
keywords = {motivation,skimmed},
mendeley-tags = {motivation,skimmed},
number = {2009},
pages = {1--14},
title = {{Tracking Users ' Clicks and Submits : Tradeoffs between User Experience and Data Loss}},
year = {2010}
}
@article{Boehm2006,
abstract = {In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise-free trends—the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two “wild card” trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering processes between now and 2025, and presents an emerging scalable spiral process model for coping with the resulting challenges and opportunities of developing 21st century software-intensive systems and systems of systems. {\textcopyright} 2006 Wiley Periodicals, Inc. Syst Eng 9: 1–19, 2006},
author = {Boehm, Barry},
doi = {10.1002/sys.20044},
file = {:Users/jk/Documents/Uni/master/refs/FutureTrendsSEProcesses.pdf:pdf},
isbn = {1520-6858},
issn = {10981241},
journal = {Systems Engineering},
keywords = {Future trends,Human systems integration,Software engineering,Spiral model,Systems acquisition,Systems architecting,Systems engineering,Systems of systems,Value-based processes,motivation},
mendeley-tags = {motivation},
number = {1},
pages = {1--19},
title = {{Some future trends and implications for systems and software engineering processes}},
volume = {9},
year = {2006}
}
@book{Robson2011,
abstract = {"This bestselling text on carrying out research in 'real world' situations has been thoroughly revised and updated but continues to have the same authoritative voice and definitive content that has made it such an essential book for teachers and students conducting research within the social sciences"--},
author = {Robson, C},
booktitle = {Chichester: Wiley},
isbn = {9781405182409},
keywords = {Dewey: 300.72,PSYCHOLOGY / Statistics,Psychology -- Research -- Methodology,Social sciences -- Research -- Methodology,evaluation},
mendeley-tags = {evaluation},
pages = {586},
title = {{Real world research : a resource for users of social research methods in applied settings}},
url = {http://library.wur.nl/WebQuery/clc/1981089},
year = {2011}
}
@article{Nichols1997,
abstract = {Social filtering systems that use explicit ratings require a large number of ratings to remain viable. The effort involved for a user to rate a document may outweigh any benefit received, leading to a shortage of ratings. One approach to this problem is to use implicit ratings: where user actions are recorded and a rating is inferred from the recorded data. This paper discusses the costs and benefits of using implicit ratings for information filtering applications.},
archivePrefix = {arXiv},
arxivId = {91},
author = {Nichols, David M},
doi = {citeulike-article-id:2583460},
eprint = {91},
isbn = {2912335043},
journal = {World Wide Web Internet And Web Information Systems},
keywords = {based on,cash,economic filtering will become,few actively-used systems,has,increasingly important as digital,internet,micro-payments and secure,moved on from the,of the importance of,original description,payment technologies emerge from,research laboratories onto the,research projects and a,sender of a message,social filtering,systems perform is largely,the identity of the,the social filtering these,the third form,to several},
pages = {31--36},
title = {{Implicit Rating and Filtering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5375{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@incollection{Boehm2011,
abstract = {This paper provides an update and extension of a 2006 paper, “Some Future Trends and Implications for Systems and Software Engineering Processes,” Systems Engineering, Spring 2006. Some of its challenges and opportunities are similar, such as the need to simultaneously achieve high levels of both agility and assurance. Others have emerged as increasingly important, such as the challenges of dealing with ultralarge volumes of data, with multicore chips, and with software as a service. The paper is organized around eight relatively surprise-free trends and two “wild cards” whose trends and implications are harder to foresee. The eight surprise-free trends are: 1. Increasing emphasis on rapid development and adaptability; 2. Increasing software criticality and need for assurance; 3. Increased complexity, global systems of systems, and need for scalability and interoperability; 4. Increased needs to accommodate COTS, software services, and legacy systems; 5. Increasingly large volumes of data and ways to learn from them; 6. Increased emphasis on users and end value; 7. Computational plenty and multicore chips; 8. Increasing integration of software and systems engineering; The two wild-card trends are: 9. Increasing software autonomy; and 10. Combinations of biology and computing.},
author = {Boehm, Barry},
booktitle = {The Future of Software Engineering},
doi = {10.1007/978-3-642-15187-3_1},
isbn = {9783642151866},
issn = {9783642151866},
pages = {1--32},
title = {{Some future software engineering opportunities and challenges}},
year = {2011}
}
@article{Boehm2006a,
abstract = {In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise-free trends—the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two “wild card” trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering processes between now and 2025, and presents an emerging scalable spiral process model for coping with the resulting challenges and opportunities of developing 21st century software-intensive systems and systems of systems. {\textcopyright} 2006 Wiley Periodicals, Inc. Syst Eng 9: 1–19, 2006},
annote = {Discusses future trends in software engineering, esp. "rapid change"; good ref. for motivation and introduction},
author = {Boehm, Barry},
doi = {10.1002/sys.20044},
file = {:Users/jk/Documents/Uni/master/refs/FutureTrendsSEProcesses.pdf:pdf},
isbn = {1520-6858},
issn = {10981241},
journal = {Systems Engineering},
keywords = {Future trends,Human systems integration,Software engineering,Spiral model,Systems acquisition,Systems architecting,Systems engineering,Systems of systems,Value-based processes,experimentation,introduction,motivation},
mendeley-tags = {experimentation,introduction,motivation},
number = {1},
pages = {1--19},
title = {{Some future trends and implications for systems and software engineering processes}},
volume = {9},
year = {2006}
}
@article{Kohavi2013,
abstract = {Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Inuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 100 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are millions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up- front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1{\%} improvement to revenue equals {\$}10M annually in the US, yet many ideas impact key metrics by 1{\%} and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.},
annote = {How Microsoft does experimentation; lessons learned, culture, implementation, analysis - should read more},
author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
doi = {10.1145/2487575.2488217},
file = {:Users/jk/Documents/Uni/master/refs/kohavi2013.pdf:pdf},
isbn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
keywords = {8,9,a,agile development,and zynga,b testing,controlled experiments,controlled experiments are especially,example,introduction,online experiments,read more,related work,s,search,steve blank,useful in combination with,yahoo},
mendeley-tags = {example,introduction,read more,related work},
number = {August},
pages = {1168},
title = {{Online controlled experiments at large scale}},
url = {http://dl.acm.org/citation.cfm?id=2488217{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2487575.2488217},
year = {2013}
}
@article{Kleinberg1999,
abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authoritative ” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages ” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Kleinberg, Jon M.},
doi = {10.1145/324133.324140},
eprint = {0208024},
isbn = {0898714109},
issn = {00045411},
journal = {Journal of the ACM},
number = {5},
pages = {604--632},
pmid = {17113247},
primaryClass = {gr-qc},
title = {{Authoritative sources in a hyperlinked environment}},
url = {http://portal.acm.org/citation.cfm?doid=324133.324140},
volume = {46},
year = {1999}
}
@article{Fagerholm2014,
abstract = {Development of software-intensive products and services increasingly$\backslash$r$\backslash$noccurs by continuously deploying product or service increments,$\backslash$r$\backslash$nsuch as new features and enhancements, to customers. Product$\backslash$r$\backslash$nand service developers need to continuously find out what customers$\backslash$r$\backslash$nwant by direct customer feedback and observation of usage$\backslash$r$\backslash$nbehaviour, rather than indirectly through up-front business analyses.$\backslash$r$\backslash$nThis paper examines the preconditions for setting up an experimentation$\backslash$r$\backslash$nsystem for continuous customer experiments. It describes the$\backslash$r$\backslash$nbuilding blocks required for such a system. An initial model for continuous$\backslash$r$\backslash$nexperimentation is analytically derived from prior work. The$\backslash$r$\backslash$nmodel is then matched against empirical case study findings from$\backslash$r$\backslash$na startup company and adjusted. Building blocks for a continuous$\backslash$r$\backslash$nexperimentation system and infrastructure are presented. A suitable$\backslash$r$\backslash$nexperimentation system requires at least the ability to release$\backslash$r$\backslash$nminimum viable products or features with suitable instrumentation,$\backslash$r$\backslash$ndesign and manage experiment plans, link experiment results with a$\backslash$r$\backslash$nproduct roadmap, and manage a flexible business strategy. The main$\backslash$r$\backslash$nchallenges are proper and rapid design of experiments, advanced$\backslash$r$\backslash$ninstrumentation of software to collect, analyse, and store relevant$\backslash$r$\backslash$ndata, and the integration of experiment results in both the product$\backslash$r$\backslash$ndevelopment cycle and the software development process},
annote = {Describes a theoretical framework for an experimentation infrastructure (organization-wide)
-could use this as a building block for my experimentation infrastructure, or at least in the fundamentals / related work},
author = {Fagerholm, Fabian and Guinea, Alejandro Sanchez and M{\"{a}}enp{\"{a}}{\"{a}}, Hanna and M{\"{u}}nch, J{\"{u}}rgen},
doi = {10.1145/2593812.2593816},
file = {:Users/jk/Documents/Uni/master/refs/fagerholm2014.pdf:pdf},
isbn = {9781450328562},
journal = {Proceedings of the 1st International Workshop on Rapid Continuous Software Engineering},
keywords = {agile software development,architecture,continuous experimentation,experimentation,fundamentals,lean,lean software development,product development,related work},
mendeley-tags = {experimentation,fundamentals,related work},
number = {June},
pages = {26--35},
title = {{Building Blocks for Continuous Experimentation}},
year = {2014}
}
@article{Amatriain2013,
abstract = {Since the Netflix 1 million Prize, announced in 2006, Netflix has been known for having personalization at the core of our product. Our current product offering is nowadays focused around instant video streaming, and our data is now many orders of magnitude larger. Not only do we have many more users in many more countries, but we also receive many more streams of data. Besides the ratings, we now also use information such as what our members play, browse, or search. In this paper I will discuss the different approaches we follow to deal with these large streams of user data in order to extract information for personalizing our service. I will describe some of the machine learning models used, and their application in the service. I will also describe our data-driven approach to innovation that combines rapid offline explorations as well as online A/B testing. This approach enables us to convert user information into real and measurable business value. Copyright 2013 ACM.},
annote = {Reference for the introduction

-some information about how netflix does A/B tests
-offline tests as cheaper alternatives to A/B tests; uses machine learning techniques (could mention this some where in the thesis, but should not go into detail)},
author = {Amatriain, Xavier},
doi = {10.1145/2505515.2514701},
file = {:Users/jk/Documents/Uni/master/refs/amatriain2013.pdf:pdf},
isbn = {9781450322638},
journal = {Proceedings of the 22nd ACM international conference on Conference on information {\&} knowledge management - CIKM '13},
keywords = {machine learning,personalization,recommender systems},
pages = {2201--2208},
title = {{Beyond Data : From User Information to Business Value through Personalized Recommendations and Consumer Science Categories and Subject Descriptors}},
url = {http://dl.acm.org/citation.cfm?doid=2505515.2514701},
year = {2013}
}
@article{Adams2013,
abstract = {Pipeline is a new development process at Adobe designed to rapidly prototype and evaluate new product offerings. Pipeline has user research at its core, and success is defined by how much is learned about a given problem, not by how much product is built. Starting ideas for new product directions are identified through Contextual Inquiry. Once a product direction is selected, an iterative process of development and evaluation is carried out over a 13-week period. Opportunities to pivot are built in at 3-week intervals, driven by evaluation results from laboratory studies. The Pipeline process is explained through an example product prototype, called "Gadget". Gadget is an application targeted at Web developers that helps them more easily experiment with and modify the visual layout of a Web page.},
annote = {Basically just says "we do cont. innovation at Adobe"; useful as a reference in the introduction},
author = {Adams, RJ and Evans, B and Brandt, J},
doi = {10.1145/2468356.2468758},
file = {:Users/jk/Documents/Uni/master/refs/adams2013.pdf:pdf},
isbn = {9781450319522},
journal = {CHI'13 Extended Abstracts},
keywords = {introduction},
mendeley-tags = {introduction},
pages = {2--3},
title = {{Creating small products at a big company: adobe's pipeline innovation process}},
url = {http://dl.acm.org/citation.cfm?id=2468758},
year = {2013}
}
@article{Tang2010,
abstract = {At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experi- ence. Such changes include not only obvious user-visible changes such as modifications to a user interface, but also more subtle changes such as different machine learning algorithms that might affect rank- ing or content selection. Our insatiable appetite for experimenta- tion has led us to tackle the problems of how to run more experi- ments, how to run experiments that produce better decisions, and howto run them faster. In this paper, we describe Google's overlap- ping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufficient, we also discuss the associated tools and ed- ucational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper specifically describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.},
annote = {Descsribes how Google implements a "overlapping
experiment infrastructure"
-more technical than the other Google-paper
-read in more detail when developing the experimentation architecture / implementing experiments},
author = {Tang, Diane and Agarwal, Ashish and O'Brien, D and Meyer, Mike},
doi = {10.1145/1835804.1835810},
file = {:Users/jk/Documents/Uni/master/refs/tang2010.pdf:pdf},
isbn = {9781450300551},
issn = {1450300553},
journal = {Proceedings of the 16th ACM {\ldots}},
keywords = {a,b testing,controlled experiments,example,experiment-infrastructure,google,implementation,introduction,multivari-,read more,related work,skimmed,website testing},
mendeley-tags = {example,implementation,introduction,read more,skimmed,related work},
pages = {17--26},
title = {{Overlapping experiment infrastructure: More, better, faster experimentation}},
url = {http://dl.acm.org/citation.cfm?id=1835810},
year = {2010}
}
@article{Steiber2013,
abstract = {Purpose – History is full of companies that were once innovative leaders but lost their innovative ability. The purpose of this paper is to explore, from a firm-level perspective, organizational characteristics for continuous innovation in rapidly changing industries. Design/methodology/approach – Findings from 28 interviews at Google Inc., are compared to previous research on organizational characteristics for continuous innovation. Findings – Google's organization can be viewed as a dynamic and open corporate system for continuous innovation, involving the entire organization and supported by an innovation-oriented and change-prone top management and board. The relative importance of eight organizational characteristics in this corporate system is elaborated upon. Research limitations/implications – There is a need for empirical research contributing to the development of a more comprehensive analytical framework for continuous innovation, including the role of culture and selection/facilitation of self-organizing individuals in innovation processes; and to study how to organize for both continuous innovation and continuous improvements. Practical implications – The importance of factors such as culture and the selection of individuals, identified in the empirical study, needs to be considered by managers, and might influence their understanding of how to sustain continuous innovation over time. Originality/value – This paper provides, from a firm-level perspective and based on a unique access to empirical data, increased understanding of organizational characteristics conducive to continuous innovation in rapidly changing industries, and highlights the importance of characteristics that received less emphasis in previous research literature.},
annote = {How Google does cont. innovation; probably very interesting read but not particularly useful for my paper. Use as a reference in the introduction {\'{a}} la "Several companies, such as Google [13], ... employ continuous innovation / experimentation in their companies"},
author = {Steiber, Annika and Al{\"{a}}nge, Sverker},
doi = {10.1108/14601061311324566},
file = {:Users/jk/Documents/Uni/master/refs/steiber2013.pdf:pdf},
isbn = {1460-1060},
issn = {1460-1060},
journal = {European Journal of Innovation Management},
keywords = {interesting,introduction,skimmed},
mendeley-tags = {interesting,introduction,skimmed},
number = {2},
pages = {243--264},
title = {{A corporate system for continuous innovation: the case of Google Inc.}},
url = {http://www.emeraldinsight.com/doi/10.1108/14601061311324566},
volume = {16},
year = {2013}
}
@article{article,
author = {Davenport, Thomas},
journal = {Strategic Direction},
title = {{How to Design Smart Business Experiments}},
volume = {25},
year = {2009}
}
@article{Olsson2014,
abstract = {This book provides essential insights on the adoption of modern software engineering practices at large companies producing software-intensive systems, where hundreds or even thousands of engineers collaborate to deliver on new systems and new versions of already deployed ones. It is based on the findings collected and lessons learned at the Software Center (SC), a unique collaboration between research and industry, with Chalmers University of Technology, Gothenburg University and Malm{\"{o}} University as academic partners and Ericsson, AB Volvo, Volvo Car Corporation, Saab Electronic Defense Systems, Grundfos, Axis Communications, Jeppesen (Boeing) and Sony Mobile as industrial partners. The 17 chapters present the "Stairway to Heaven" model, which represents the typical evolution path companies move through as they develop and mature their software engineering capabilities. The chapters describe theoretical frameworks, conceptual models and, most importantly, the industrial experiences gained by the partner companies in applying novel software engineering techniques. The book's structure consists of six parts. Part I describes the model in detail and presents an overview of lessons learned in the collaboration between industry and academia. Part II deals with the first step of the Stairway to Heaven, in which R{\&}D adopts agile work practices. Part III of the book combines the next two phases, i.e., continuous integration (CI) and continuous delivery (CD), as they are closely intertwined. Part IV is concerned with the highest level, referred to as "R{\&}D as an innovation system," while Part V addresses a topic that is separate from the Stairway to Heaven and yet critically important in large organizations: organizational performance metrics that capture data, and visualizations of the status of software assets, defects and teams. Lastly, Part VI presents the perspectives of two of the SC partner companies. The book is intended for practitioners and professionals in the software-intensive systems industry, providing concrete models, frameworks and case studies that show the specific challenges that the partner companies encountered, their approaches to overcoming them, and the results. Researchers will gain valuable insights on the problems faced by large software companies, and on how to effectively tackle them in the context of successful cooperation projects.},
annote = {Focuses more on *active* user feedback --{\textgreater} out of scope; maybe usable as related work?},
author = {Olsson, Helena Holmstr{\"{o}}m and Bosch, Jan},
doi = {10.1007/978-3-319-11283-1-12},
file = {:Users/jk/Documents/Uni/master/refs/olsson2014.pdf:pdf},
isbn = {9783319112831},
issn = {18651348},
journal = {Continuous software engineering},
keywords = {out-of-scope,related work},
mendeley-tags = {out-of-scope,related work},
number = {February 2016},
pages = {143--154},
title = {{Post-deployment data collection in software-intensive embedded products}},
volume = {9783319112},
year = {2014}
}
@book{unknown,
author = {M{\"{u}}nch, J{\"{u}}rgen and Fagerholm, Fabian and Johnson, Patrik and Pirttilahti, Janne and Torkkel, Juha and J{\"{a}}rvinen, Janne},
doi = {10.1007/978-3-642-44930-7},
file = {:Users/jk/Documents/Uni/master/refs/munch2013.pdf:pdf},
isbn = {978-3-642-44929-1},
keywords = {interesting,out-of-scope,skimmed},
mendeley-tags = {interesting,out-of-scope,skimmed},
number = {May 2014},
title = {{Creating Minimum Viable Products in Industry-Academia Collaborations}},
url = {http://link.springer.com/10.1007/978-3-642-44930-7},
volume = {167},
year = {2013}
}
@article{Royce1970,
abstract = {I am going to describe my personal views about managing large software developments. I have had various assignments during the past nine years, mostly concerned with the development of software packages for spacecraft mission planning, commanding and post-flight analysis. In these assignments I have experienced different degrees of successwith respect to arriving at an operational state, on-time, and within costs. I have become prejudiced by my experiences and I am going to relate some of these prejudices in this presentation.},
author = {Royce, Winston W.},
isbn = {0897912160},
journal = {Proceedings of IEEE WESCON},
number = {8},
pages = {328--338},
title = {{Managing the Development of Large Software Systems}},
url = {http://leadinganswers.typepad.com/leading{\_}answers/files/original{\_}waterfall{\_}paper{\_}winston{\_}royce.pdf},
volume = {26},
year = {1970}
}
@book{unknown,
author = {Olsson, Helena Holmstrom and Bosch, Jan},
doi = {10.1007/978-3-642-44930-7},
file = {:Users/jk/Documents/Uni/master/refs/olsson2013.pdf:pdf},
isbn = {978-3-642-44929-1},
keywords = {fundamentals,motivation},
mendeley-tags = {fundamentals,motivation},
number = {March 2016},
pages = {152--164},
title = {{Towards Data-Driven Product Development: A Multiple Case Study on Post-Deployment Data Usage in Software-Intensive Embedded Systems}},
url = {http://link.springer.com/10.1007/978-3-642-44930-7},
volume = {167},
year = {2013}
}
@article{Jansen2010,
annote = {-Reference in "Software development as an experiment system"
-no idea how this could be relevant apart from that},
author = {Jansen, Harrie},
file = {:Users/jk/Documents/Uni/master/refs/jansen2010.pdf:pdf},
isbn = {1438-5627},
issn = {14385627},
journal = {Forum: Qualitative Social Research},
keywords = {analysis,combinatory,diversity analysis,diversity sample,holistic synthesis,out-of-scope,qualitative survey,skimmed,statistical survey,typology construction},
mendeley-tags = {out-of-scope,skimmed},
number = {2},
pages = {1--21},
pmid = {58650744},
title = {{The Logic of Qualitative Survey Research and its Position in the Field of Social Research Methods 2 . The Qualitative Survey}},
volume = {11},
year = {2010}
}
@book{Miles1994,
abstract = {most useful for data analysis--how to develop matricies and visual displays of data; and for its checklists on validitiy and reliability also good for housekeeping and data record issues of how to keep a handle on data},
annote = {(Only chapter 11! Maybe try to get whole book)

Use as a reference for designing experiments and their analysis; look at papers that reference this book in order to find where the meat is},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Miles, M. B. and Huberman, A.M.},
booktitle = {Sage},
doi = {10.1136/ebnurs.2011.100352},
eprint = {arXiv:1011.1669v3},
file = {:Users/jk/Documents/Uni/master/refs/review/p2227-xu.pdf:pdf},
isbn = {0803946538},
issn = {1367-6539},
keywords = {analysis,evaluation,fundamentals,skimmed},
mendeley-tags = {analysis,evaluation,fundamentals,skimmed},
pages = {1--354},
pmid = {22184739},
title = {{Qualitative data analysis}},
url = {http://www.rds-yh.nihr.ac.uk/wp-content/uploads/2013/05/9{\_}Qualitative{\_}Data{\_}Analysis{\_}Revision{\_}2009.pdf},
year = {1994}
}
@article{Baird2012,
abstract = {An increasing number of information systems projects in industry are managed using hybrid project management methodologies, but this shift in project management methods is not fully represented in our CIS curriculums. CIS capstone courses often include an applied project that is managed with traditional project management methods (plan first, execute second). While agile methods (adapt to change through iterations) are making inroads, little research has been conducted on using a hybrid of these two project management methods in a capstone course. In this paper, we explain the hybrid project management methods we used in four sections of an undergraduate CIS Capstone course during the Fall and Spring of the 2011-2012 academic year. We also present the results of an end-of-term student satisfaction and critical success factor survey. We find that overall satisfaction with the hybrid approach is high among our sample. We also find that more client involvement and a pragmatic approach to initial project planning are areas for future improvement. The results of our experience and survey provide lessons learned and best practices for those who wish to provide students with applied experience that combines waterfall (traditional) and Scrum (agile) project management techniques in their own courses. [PUBLICATION ABSTRACT]},
author = {Baird, Aaron and Riggins, Frederick J},
doi = {10.1002/spip},
file = {:Users/jk/Documents/Uni/master/refs/review/fogelstr{\"{o}}m2010.pdf:pdf},
isbn = {10553096},
issn = {10553096},
journal = {Journal of Information Systems Education},
keywords = {Curricula,Education--Computer Applications,Higher education,Information systems,Project management,Student attitudes,Success factors,out-of-scope},
mendeley-tags = {out-of-scope},
number = {3},
pages = {243--257},
pmid = {1315921600},
title = {{Planning and Sprinting: Use of a Hybrid Project Management Methodology within a CIS Capstone Course}},
url = {http://search.proquest.com/docview/1315921600?accountid=14719{\%}5Cnhttp://openurl.uquebec.ca:9003/uqam?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=article{\&}sid=ProQ:ProQ:abiglobal{\&}atitle=Planning+and+Sprinting:+Use+of+a+Hybrid+Project},
volume = {23},
year = {2012}
}
@misc{Runeson2008,
abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Runeson, Per and H{\"{o}}st, Martin},
booktitle = {Empirical Software Engineering},
doi = {10.1007/s10664-008-9102-8},
eprint = {9809069v1},
isbn = {1382325615737616},
issn = {1382-3256, 1573-7616},
number = {2},
pages = {131--164},
pmid = {28843849},
primaryClass = {arXiv:gr-qc},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/article/10.1007/s10664-008-9102-8{\%}5Cnhttp://link.springer.com/content/pdf/10.1007{\%}2Fs10664-008-9102-8.pdf},
volume = {14},
year = {2008}
}
@article{Fogelstrom2010,
abstract = {Agile development methods such as extreme programming (XP), SCRUM, Lean Software Development (Lean SD) and others have gained much popularity during the last years. Agile methodologies promise faster time-to-market, satisfied customers and high quality software. While these prospects are appealing, the suitability of agile practices to different domains and business contexts still remains unclear. In this article we investigate the applicability of agile principles in the context of market-driven software product development (MDPD), focusing on pre-project activities. This article presents results of a comparison between typical properties of agile methods to the needs of MDPD, as well as findings of a case study conducted at Ericsson, an early adopter of agile product development. The results show misalignment between the agile principles and needs of pre-project activities in market-driven development. This misalignment threatens to subtract from the positive aspects of agile development, but maybe more importantly, threaten the overall product development by disabling effective product management. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
author = {Fogelstr{\"{o}}m, Nina Dzamashvili and Svahnberg, Tony Gorschek Mikael and Olsson, Peo},
doi = {10.1002/smr.453},
isbn = {1532-0618},
issn = {1532060X},
journal = {Journal of Software Maintenance and Evolution},
keywords = {Agile methods,Case study,Market-driven software development,Software product management},
number = {1},
pages = {53--80},
pmid = {1315921600},
title = {{The impact of agile principles on market-driven software product development}},
volume = {22},
year = {2010}
}
@inproceedings{boehm2006view,
author = {Boehm, Barry},
booktitle = {Proceedings of the 28th international conference on Software engineering},
file = {:Users/jk/Documents/Uni/master/refs/review/boehm2006view.pdf:pdf},
keywords = {motivation},
mendeley-tags = {motivation},
organization = {ACM},
pages = {12--29},
title = {{A view of 20th and 21st century software engineering}},
year = {2006}
}
@inproceedings{lindgren2015software,
author = {Lindgren, Eveliina and M{\"{u}}nch, J{\"{u}}rgen},
booktitle = {International Conference on Agile Software Development},
file = {:Users/jk/Documents/Uni/master/refs/review/lindgren2015software.pdf:pdf},
keywords = {evaluation,experimentation,fundamentals},
mendeley-tags = {evaluation,experimentation,fundamentals},
organization = {Springer},
pages = {117--128},
title = {{Software development as an experiment system: a qualitative survey on the state of the practice}},
year = {2015}
}
@article{Deka2016,
abstract = {Figure 1: ERICA, a scalable system for interaction mining Android applications, captures interaction traces as users interact with an Android app. These traces contain snapshots of the app's UI over time (as screenshots and view hierarchies) as well as user interaction data. Here a search flow is highlighted within an interaction trace from the Amazon shopping app. ABSTRACT Design plays an important role in adoption of apps. App de-sign, however, is a complex process with multiple design ac-tivities. To enable data-driven app design applications, we present interaction mining – capturing both static (UI lay-outs, visual details) and dynamic (user flows, motion details) components of an app's design. We present ERICA, a sys-tem that takes a scalable, human-computer approach to in-teraction mining existing Android apps without the need to modify them in any way. As users interact with apps through ERICA, it detects UI changes, seamlessly records multiple data-streams in the background, and unifies them into a user interaction trace (Figure 1). Using ERICA we collected in-teraction traces from over a thousand popular Android apps. Leveraging this trace data, we built machine learning classi-fiers to detect elements and layouts indicative of 23 common user flows. User flows are an important component of user ex-perience (UX) design and consists of a sequence of UI states that represent semantically meaningful tasks such as search-ing or composing. With these classifiers, we identified and in-dexed more than 3000 flow examples, and released the largest online search engine of user flows in Android apps.},
annote = {- tool for collection of usage data for android apps
- out of scope, but could be mentioned in future work as a possibility for further improving the testing infrastructure},
author = {Deka, Biplab and Huang, Zifeng and Kumar, Ranjitha},
doi = {10.1145/2984511.2984581},
file = {:Users/jk/Documents/Uni/master/refs/review/deka-uist2016-erica.pdf:pdf},
isbn = {978-1-4503-4189-9},
journal = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
keywords = {app design,design mining,future work,interaction mining,out-of-scope,skimmed,user flows},
mendeley-tags = {future work,out-of-scope,skimmed},
pages = {767--776},
title = {{ERICA: Interaction Mining Mobile Apps}},
year = {2016}
}
@inproceedings{drutsa2015future,
author = {Drutsa, Alexey and Gusev, Gleb and Serdyukov, Pavel},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
file = {:Users/jk/Documents/Uni/master/refs/review/p256.pdf:pdf},
keywords = {fundamentals,skimmed},
mendeley-tags = {fundamentals,skimmed},
organization = {International World Wide Web Conferences Steering Committee},
pages = {256--266},
title = {{Future user engagement prediction and its application to improve the sensitivity of online experiments}},
year = {2015}
}
@article{Runeson2009,
abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
annote = {-details about the terminology etc. of case studies, but only mentions experiments briefly},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Runeson, Per and H{\"{o}}st, Martin},
doi = {10.1007/s10664-008-9102-8},
eprint = {9809069v1},
file = {:Users/jk/Documents/Uni/master/refs/review/runeson2008.pdf:pdf},
isbn = {1382325615737616},
issn = {13823256},
journal = {Empirical Software Engineering},
keywords = {Case study,Checklists,Guidelines,Research methodology,out-of-scope},
mendeley-tags = {out-of-scope},
number = {2},
pages = {131--164},
pmid = {28843849},
primaryClass = {arXiv:gr-qc},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/article/10.1007/s10664-008-9102-8{\%}5Cnhttp://link.springer.com/content/pdf/10.1007{\%}2Fs10664-008-9102-8.pdf},
volume = {14},
year = {2009}
}
@article{Williams2003,
abstract = {Currently, the focus is on determining how to blend agile methodologies with plan-driven approaches to software development.},
annote = {Useful as a reference in the introduction when reasoning about the need for fast adaptation to changing customer {\&} market needs},
author = {Williams, Laurie and Cockburn, Alistair},
doi = {10.1109/MC.2003.1204373},
file = {:Users/jk/Documents/Uni/master/refs/review/williams2003.pdf:pdf},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
keywords = {introduction,skimmed},
mendeley-tags = {introduction,skimmed},
number = {6},
pages = {39--43},
pmid = {21692106},
title = {{Agile software development: It's about feedback and change}},
volume = {36},
year = {2003}
}
@article{Bosch2012,
abstract = {Traditional software development focuses on specifying and freezing requirements early in the, typically yearly, product development lifecycle. The requirements are defined based on product management's best understanding. The adoption of SaaS and cloud computing has shown a different approach to managing requirements, adding frequent and rigorous experimentation to the development process with the intent of minimizing R{\&}D investment between customer proof points. This offers several benefits including increased customer satisfaction, improved and quantified business goals and the transformation to a continuous rather than waterfall development process. In this paper, we present our learnings from studying software companies applying an innovation experiment system approach to product development. The approach is illustrated with three cases from Intuit, the case study company. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
author = {Bosch, Jan},
doi = {10.1007/978-3-642-30746-1_3},
file = {:Users/jk/Documents/Uni/master/refs/review/bosch2012.pdf:pdf},
isbn = {9783642307454},
issn = {18651348},
journal = {Lecture Notes in Business Information Processing},
keywords = {Product development approach,case study,experiment systems,experimentation,motivation},
mendeley-tags = {experimentation,motivation},
number = {Icsob},
pages = {27--39},
title = {{Building products as innovation experiment systems}},
volume = {114 LNBIP},
year = {2012}
}
@article{Barbula2014,
author = {Barbula, Alexander},
file = {:Users/jk/Documents/Uni/master/refs/review/barbula2014.pdf:pdf},
title = {{Automatisierungsm{\"{o}}glichkeiten von Usability Evaluationen im Kontext des Mobile Application Testings}},
year = {2014}
}
@article{Fogelstrom2010a,
abstract = {Agile development methods such as extreme programming (XP), SCRUM, Lean Software Development (Lean SD) and others have gained much popularity during the last years. Agile methodologies promise faster time-to-market, satisfied customers and high quality software. While these prospects are appealing, the suitability of agile practices to different domains and business contexts still remains unclear. In this article we investigate the applicability of agile principles in the context of market-driven software product development (MDPD), focusing on pre-project activities. This article presents results of a comparison between typical properties of agile methods to the needs of MDPD, as well as findings of a case study conducted at Ericsson, an early adopter of agile product development. The results show misalignment between the agile principles and needs of pre-project activities in market-driven development. This misalignment threatens to subtract from the positive aspects of agile development, but maybe more importantly, threaten the overall product development by disabling effective product management. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
author = {Fogelstr{\"{o}}m, Nina Dzamashvili and Svahnberg, Tony Gorschek Mikael and Olsson, Peo},
doi = {10.1002/smr.453},
file = {:Users/jk/Documents/Uni/master/refs/review/fogelstr{\"{o}}m2010.pdf:pdf},
isbn = {1532-0618},
issn = {1532060X},
journal = {Journal of Software Maintenance and Evolution},
keywords = {Agile methods,Case study,Market-driven software development,Software product management},
number = {1},
pages = {53--80},
pmid = {1315921600},
title = {{The impact of agile principles on market-driven software product development}},
url = {http://search.proquest.com/docview/1315921600?accountid=14719{\%}5Cnhttp://openurl.uquebec.ca:9003/uqam?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=article{\&}sid=ProQ:ProQ:abiglobal{\&}atitle=Planning+and+Sprinting:+Use+of+a+Hybrid+Project},
volume = {22},
year = {2010}
}
@article{Song2010,
abstract = {Query suggestion has been an effective approach to help users narrow down to the information they need. However, most of existing studies focused on only popular/head queries. Since rare queries possess much less information (e.g., clicks) than popular queries in the query logs, it is much more difficult to efficiently suggest relevant queries to a rare query. In this paper, we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs. Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant. However, we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently. Hence, our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation. Our model specifically optimizes two parameters: (1) the restarting (jumping) rate of random walk, and (2) the combination ratio of click and skip information. Unlike the Rocchio algorithm, our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query- URL bipartite graphs. Consequently, our model is capable of scaling up to the need of commercial search engines. Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework, with statistical significance, over the traditional random walk models and pseudo-relevance feedback models.},
annote = {Very technical paper describing an algorithm that rates user queries (e.g. in a search engine) using implicit user feedback (such as clicked / skipped URLs)

Does not seem very relevant for me, except as an example of an implicit user feedback application},
author = {Song, Yang and He, Li-wei},
doi = {10.1145/1772690.1772782},
file = {:Users/jk/Documents/Uni/master/refs/review/p901-song.pdf:pdf},
isbn = {9781605587998},
journal = {Proceedings of the 19th international conference on World wide web},
keywords = {pseudo-relevance feedback,query suggestion,random walk,technical},
mendeley-tags = {technical},
pages = {901--910},
title = {{Optimal rare query suggestion with implicit user feedback}},
url = {http://dl.acm.org/citation.cfm?id=1772782},
year = {2010}
}
@article{Sharma2005,
abstract = {Measuring the information retrieval effectiveness of Web search engines can be expensive if human relevance judgments are required to evaluate search results. Using implicit user feedback for search engine evaluation provides a cost and time effective manner of addressing this problem. Web search engines can use human evaluation of search results without the expense of human evaluators. An additional advantage of this approach is the availability of real time data regarding system performance. Wecapture user relevance judgments actions such as print, save and bookmark, sending these actions and the corresponding document identifiers to a central server via a client application. We use this implicit feedback to calculate performance metrics, such as precision. We can calculate an overall system performance metric based on a collection of weighted metrics. {\textcopyright} 2005 ACM.},
annote = {Technical paper about the implementation of an implicit feedback capture and analysis program - but very short, not precise, and quite outdated

--{\textgreater} probably not useful},
author = {Sharma, H. and Jansen, B.J.},
doi = {10.1145/1076034.1076172},
file = {:Users/jk/Documents/Uni/master/refs/review/p649-sharma.pdf:pdf},
isbn = {1595930345 | 9781595930347},
journal = {SIGIR 2005 - Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
keywords = {[implicit user feedback,implementation,search engine evaluation],technical},
mendeley-tags = {implementation,technical},
number = {1},
pages = {649--650},
title = {{Automated evaluation of search engine performance via implicit user feedback}},
year = {2005}
}
@article{Rendle2009,
abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
annote = {Describes BPR algorithm for recommendation systems; very technical {\&} probably out of scope},
archivePrefix = {arXiv},
arxivId = {1205.2618},
author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
doi = {10.1145/1772690.1772773},
eprint = {1205.2618},
file = {:Users/jk/Documents/Uni/master/refs/review/p452-rendle.pdf:pdf},
isbn = {978-0-9749039-5-8},
issn = {1469493X},
journal = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
keywords = {out-of-scope,technical},
mendeley-tags = {out-of-scope,technical},
pages = {452--461},
pmid = {21975771},
title = {{BPR: Bayesian Personalized Ranking from Implicit Feedback}},
url = {http://dl.acm.org/citation.cfm?id=1795114.1795167},
year = {2009}
}
@article{Radlinski2005,
abstract = {This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.},
annote = {-Probably too technical
-concerned with clickthrough data, i.e. too low-level (?)
-maybe read up on details if needed},
archivePrefix = {arXiv},
arxivId = {cs/0605035},
author = {Radlinski, Filip and Joachims, Thorsten},
doi = {10.1145/1081870.1081899},
eprint = {0605035},
file = {:Users/jk/Documents/Uni/master/refs/review/p239-radlinski.pdf:pdf},
isbn = {159593135X},
issn = {01482963},
journal = {Kdd},
keywords = {clickthrough data,implicit feedback,machine learning,port vector machines,search engines,skimmed,sup-},
mendeley-tags = {skimmed},
pages = {10},
primaryClass = {cs},
title = {{Query Chains: Learning to Rank from Implicit Feedback}},
url = {http://arxiv.org/abs/cs/0605035},
year = {2005}
}
@article{Joachims2005,
abstract = {This paper examines the reliability of implicit feedback generated from clickthrough data in WWW search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences de- rived from clicks are reasonably accurate on average.},
author = {Joachims, Thorsten and Granka, Laura and Pan, Bing and Hembrooke, Helene and Gay, Geri},
doi = {10.1145/1076034.1076063},
file = {:Users/jk/Documents/Uni/master/refs/review/p154-joachims.pdf:pdf},
isbn = {1595930345},
journal = {Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '05},
keywords = {clickthrough,evaluation,eyetracking,implicit feedback,www search},
mendeley-tags = {evaluation},
pages = {154},
pmid = {2342},
title = {{Accurately interpreting clickthrough data as implicit feedback}},
url = {http://dl.acm.org/citation.cfm?doid=1076034.1076063},
year = {2005}
}
@article{Kabbedijk2013,
abstract = {In order to maximize their customer base, business software vendors are trying to offer a software products that supports the business needs of as many customers as possible. The more standardized a software product is, the easier it will be to serve large numbers of uniform customers. However, if customers are not homogeneous, a trade-off must be made between flexibility and complexity. A case study is presented showing the implementation of the Command Query Responsibility Pattern (CQRS), a pattern dictating the strict separation between commands and queries. The study was performed at a large software product vendor currently designing a software product based on CQRS. Seven sub patterns related to CQRS are identified and discussed within this research. The research shows how implementing the CQRS pattern and its different sub patterns can result in a high level of variability within a software product and how the different sub patterns can interact to achieve this.},
annote = {Describes how a company implements the CQRS pattern; not useful as a reference in itself, but can use its references for finding literature about and explaining CQRS and event sourcing},
author = {Kabbedijk, Jaap and JANSEN, Slinger and Brinkkemper, S},
doi = {10.1145/0000000.0000000},
file = {:Users/jk/Documents/Uni/master/refs/review/a2-kabbedijk.pdf:pdf},
isbn = {9781450301077},
journal = {Hillside.Net},
keywords = {cqrs,event sourcing,example,fundamentals},
mendeley-tags = {cqrs,event sourcing,example,fundamentals},
pages = {1--10},
title = {{A Case Study of the Variability Consequences of the CQRS Pattern in Online Business Software}},
url = {http://hillside.net/europlop/europlop2012/submission/shepherd.cgi?token=28ba4c725fb24eed97c7f6c82875432e363054ea{\&}action=download{\&}label=1375785521{\_}20},
year = {2013}
}
@article{Jawaheer2014,
abstract = {Recommender systems are firmly established as a standard technology for assisting users with their choices; however, little attention has been paid to the application of the user model in recommender systems, par- ticularly the variability and noise that are an intrinsic part of human behavior and activity. To enable recommender systems to suggest items that are useful to a particular user, it can be essential to understand the user and his or her interactions with the system. These interactions typically manifest themselves as explicit and implicit user feedback that provides the key indicators for modeling users' preferences for items and essential information for personalizing recommendations. In this article, we propose a classification framework for the use of explicit and implicit user feedback in recommender systems based on a set of dis- tinct properties that include CognitiveEffort,UserModel, Scale of Measurement, andDomainRelevance.We develop a set of comparison criteria for explicit and implicit user feedback to emphasize the key properties. Using our framework, we provide a classification of recommender systems that have addressed questions about user feedback, and we review state-of-the-art techniques to improve such user feedback and thereby improve the performance of the recommender system. Finally, we formulate challenges for future research on},
author = {Jawaheer, Gawesh and Weller, Peter and Kostkova, Patty},
doi = {10.1145/2512208},
file = {:Users/jk/Documents/Uni/master/refs/review/a8-jawaheer.pdf:pdf},
issn = {2160-6455},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {Feedback,explicit feedback,implicit feedback,improvement of feedback,recommender systems},
number = {2},
pages = {8:1----8:26},
title = {{Modeling User Preferences in Recommender Systems: A Classification Framework for Explicit and Implicit User Feedback}},
url = {http://doi.acm.org/10.1145/2512208},
volume = {4},
year = {2014}
}
@misc{,
file = {:Users/jk/Documents/Uni/master/refs/Event Sourcing Basics – Event Store.pdf:pdf},
pages = {1--11},
title = {{Event Sourcing Basics — Event Store}},
url = {http://docs.geteventstore.com/introduction/4.0.0/event-sourcing-basics/},
urldate = {2017-10-17}
}
@article{Olsson2012,
abstract = {Agile software development is well-known for its focus on close customer collaboration and customer feedback. In emphasizing flexibility, efficiency and speed, agile practices have lead to a paradigm shift in how software is developed. However, while agile practices have succeeded in involving the customer in the development cycle, there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product. The concept of continuous deployment, i.e. the ability to deliver software functionality frequently to customers and subsequently, the ability to continuously learn from real-time customer usage of software, has become attractive to companies realizing the potential in having even shorter feedback loops. However, the transition towards continuous deployment involves a number of barriers. This paper presents a multiple-case study in which we explore barriers associated with the transition towards continuous deployment. Based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these.},
author = {Olsson, Helena Holmstr{\"{o}}m and Alahyari, Hiva and Bosch, Jan},
doi = {10.1109/SEAA.2012.54},
file = {:Users/jk/Documents/Uni/master/refs/review/4790a392.pdf:pdf},
isbn = {9780769547909},
issn = {1089-6503},
journal = {Proceedings - 38th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2012},
keywords = {agile,agile software development,continuous deployment,continuous integration,customer collaboration,fundamentals,interesting,motivation},
mendeley-tags = {agile,fundamentals,interesting,motivation},
number = {January},
pages = {392--399},
title = {{Climbing the "Stairway to heaven" - A mulitiple-case study exploring barriers in the transition from agile development towards continuous deployment of software}},
year = {2012}
}
@book{evans2004domain,
author = {Evans, Eric},
keywords = {get},
mendeley-tags = {get},
publisher = {Addison-Wesley Professional},
title = {{Domain-driven design: tackling complexity in the heart of software}},
year = {2004}
}
@article{Kohavi2013a,
author = {Kohavi, Ronny},
doi = {10.1145/2512875.2517149},
file = {:Users/jk/Documents/Uni/master/refs/Online Controlled Experiments and A{\_}B Testing.pdf:pdf},
isbn = {9781450324212},
journal = {Proceedings of the 1st workshop on User engagement optimization - UEO '13},
keywords = {a/b testing,controlled experiments,experimentation,fundamentals,online experiments},
mendeley-tags = {a/b testing,experimentation,fundamentals},
pages = {15--16},
title = {{Online controlled experiments}},
url = {http://dl.acm.org/citation.cfm?id=2512875.2517149},
volume = {2},
year = {2013}
}
@article{Xu2015,
abstract = {A/B testing, also known as bucket testing, split testing, or controlled experiment, is a standard way to evaluate user engagement or satisfaction from a new service, feature, or product. It is widely used among online websites, including social network sites such as Facebook, LinkedIn, and Twitter to make data-driven decisions. At LinkedIn, we have seen tremendous growth of controlled experiments over time, with now over 400 concurrent experiments running per day. General A/B testing frameworks and methodologies, including challenges and pitfalls, have been discussed extensively in several previous KDD work [7, 8, 9, 10]. In this paper, we describe in depth the experimentation platform we have built at LinkedIn and the challenges that arise particularly when running A/B tests at large scale in a social network setting. We start with an introduction of the experimentation platform and how it is built to handle each step of the A/B testing process at LinkedIn, from designing and deploying experiments to analyzing them. It is then followed by discussions on several more sophisticated A/B testing scenarios, such as running offline experiments and addressing the network effect, where one user's action can influence that of another. Lastly, we talk about features and processes that are crucial for building a strong experimentation culture.},
annote = {Describes how Linkedin implements experimentation (in terms of code, culture, architecture, ...)

- use as a reference when creating my experimentation architecture
- probably not ideal as a reference for "hard facts" but serves as a starting point},
author = {Xu, Ya and Chen, Nanyu and Fernandez, Addrian and Sinno, Omar and Bhasin, Anmol},
doi = {10.1145/2783258.2788602},
file = {:Users/jk/Documents/Uni/master/refs/p2227-xu.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
keywords = {a/b testing,architecture,controlled experiments,example,experimentation,fundamentals,measurement,network a/b testing,online experiments,related work,social network},
mendeley-tags = {architecture,example,experimentation,fundamentals,related work},
pages = {2227--2236},
title = {{From Infrastructure to Culture}},
url = {http://doi.acm.org/10.1145/2783258.2788602{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2783258.2788602},
year = {2015}
}
@article{Chuklin2013,
abstract = {In recent years many models have been proposed that are aimed at predicting clicks of web search users. In addition, some information retrieval evaluation metrics have been built on top of a user model. In this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. We then put the resulting model-based metrics as well as traditional metrics (like DCG or Precision) into a common evaluation framework and compare them along a number of dimensions. One of the dimensions we are particularly interested in is the agreement between offline and online experimental outcomes. It is widely believed, especially in an industrial setting, that online A/B-testing and interleaving experiments are generally better at capturing system quality than offline measurements. We show that offline metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional offline metrics, especially in situations when we have incomplete relevance judgements. Copyright {\textcopyright} 2013 ACM.},
author = {Chuklin, Aleksandr and Serdyukov, Pavel and de Rijke, Maarten},
doi = {10.1145/2484028.2484071},
file = {:Users/jk/Documents/Uni/master/refs/p493-chuklin.pdf:pdf},
isbn = {9781450320344},
journal = {Sigir},
keywords = {Click models,Evaluation,Information retrieval measures,User behavior,click models,evaluation,information retrieval measures,related work},
mendeley-tags = {related work},
number = {2},
pages = {493--502},
title = {{Click model-based information retrieval metrics}},
url = {http://dl.acm.org/citation.cfm?doid=2484028.2484071},
year = {2013}
}
@article{Crook2009,
abstract = {Controlled experiments, also called randomized experiments and A/B tests, have had a profound influence on multiple fields, including medicine, agriculture, manufacturing, and advertising. While the theoretical aspects of offline controlled experiments have been well studied and documented, the practical aspects of running them in online settings, such as web sites and services, are still being developed. As the usage of controlled experiments grows in these online settings, it is becoming more important to understand the opportunities and pitfalls one might face when using them in practice. A survey of online controlled experiments and lessons learned were previously documented in Controlled Experiments on the Web: Survey and Practical Guide (Kohavi, et al., 2009). In this follow-on paper, we focus on pitfalls we have seen after running numerous experiments at Microsoft. The pitfalls include a wide range of topics, such as assuming that common statistical formulas used to calculate standard deviation and statistical power can be applied and ignoring robots in analysis (a problem unique to online settings). Online experiments allow for techniques like gradual ramp-up of treatments to avoid the possibility of exposing many customers to a bad (e.g., buggy) Treatment. With that ability, we discovered that it's easy to incorrectly identify the winning Treatment because of Simpson's paradox.},
author = {Crook, Thomas and Frasca, Brian},
doi = {10.1145/1557019.1557139},
file = {:Users/jk/Documents/Uni/master/refs/p1105-crook.pdf:pdf},
isbn = {9781605584959},
issn = {1605584959},
journal = {Proceedings of the 15th {\ldots}},
keywords = {a/b testing,details,implementation},
mendeley-tags = {a/b testing,details,implementation},
pages = {1105--1114},
title = {{Seven pitfalls to avoid when running controlled experiments on the web}},
url = {http://dx.doi.org/10.1145/1557019.1557139{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1557139},
year = {2009}
}
@article{Kelly:2003:IFI:959258.959260,
address = {New York, NY, USA},
author = {Kelly, Diane and Teevan, Jaime},
doi = {10.1145/959258.959260},
file = {:Users/jk/Documents/Uni/master/refs/p18-kelly.pdf:pdf},
issn = {0163-5840},
journal = {SIGIR Forum},
keywords = {fundamentals,implicit user feedback},
mendeley-tags = {fundamentals,implicit user feedback},
month = {sep},
number = {2},
pages = {18--28},
publisher = {ACM},
title = {{Implicit Feedback for Inferring User Preference: A Bibliography}},
url = {http://doi.acm.org/10.1145/959258.959260},
volume = {37},
year = {2003}
}
@online{WEB:Fowler:2005,
	Author = {Fowler, Martin},
	Title = {Event Sourcing},
	Url = {https://martinfowler.com/eaaDev/EventSourcing.html},
	Urldate = {2017-11-06},
	Year = {2005}
}
@online{WEB:Fowler:2011,
	Author = {Fowler, Martin},
	Title = {CQRS},
	Url = {https://martinfowler.com/bliki/CQRS.html},
	Urldate = {2017-11-06},
	Year = {2011}
}
@online{WEB:EventStore:2017,
	Author = {{Event Store LLP}},
	Title = {Event Store},
	Url = {https://eventstore.org/},
	Urldate = {2017-11-09}
}
@article{fowler2001agile,
author = {Fowler, Martin and Highsmith, Jim},
journal = {Software Development},
keywords = {introduction,motivation},
mendeley-groups = {Master Thesis},
mendeley-tags = {introduction,motivation},
number = {8},
pages = {28--35},
publisher = {[San Francisco: Miller Freeman, Inc., 1993]},
title = {{The agile manifesto}},
volume = {9},
year = {2001}
}

@book{ries2011lean,
author = {Ries, Eric},
keywords = {introduction,motivation},
mendeley-groups = {Master Thesis},
mendeley-tags = {introduction,motivation},
publisher = {Crown Books},
title = {{The lean startup: How today's entrepreneurs use continuous innovation to create radically successful businesses}},
year = {2011}
}
@book{rossi2003evaluation,
  title={Evaluation: A systematic approach},
  author={Rossi, Peter H and Lipsey, Mark W and Freeman, Howard E},
  year={2003},
  publisher={Sage publications}
}
@book{marks2000progress,
  title={The progress of experiment: science and therapeutic reform in the United States, 1900-1990},
  author={Marks, Harry M},
  year={2000},
  publisher={Cambridge University Press}
@Book{ford2017building,
    author="Ford, Neal and Parsons, Rebecca and Kua, Patrick",
    title="Building Evolutionary Architectures: Support Constant Change",
    publisher="O'Reilly Media",
    year=2017
}
@book{graham2008foundations,
  title={Foundations of software testing: ISTQB certification},
  author={Graham, Dorothy and Van Veenendaal, Erik and Evans, Isabel},
  year={2008},
  publisher={Cengage Learning EMEA}
@online{WEB:Fowler:2005-2,
	Author = {Fowler, Martin},
	Title = {Parallel Model},
	Url = {https://martinfowler.com/eaaDev/ParallelModel.html},
	Urldate = {2017-11-09},
	Year = {2005}
}
@article{roche2013adopting,
  title={Adopting DevOps practices in quality assurance},
  author={Roche, James},
  journal={Communications of the ACM},
  volume={56},
  number={11},
  pages={38--43},
  year={2013},
  publisher={ACM}
}@book{bass2015devops,
  title={DevOps: A Software Architect's Perspective},
  author={Bass, Len and Weber, Ingo and Zhu, Liming},
  year={2015},
  publisher={Addison-Wesley Professional}
}
@article{humble2011enterprises,
  title={Why enterprises must adopt devops to enable continuous delivery},
  author={Humble, Jez and Molesky, Joanne},
  journal={Cutter IT Journal},
  volume={24},
  number={8},
  pages={6},
  year={2011}
}
@book{Bosch2014,
abstract = {This book provides essential insights on the adoption of modern software engineering practices at large companies producing software-intensive systems, where hundreds or even thousands of engineers collaborate to deliver on new systems and new versions of already deployed ones. It is based on the findings collected and lessons learned at the Software Center (SC), a unique collaboration between research and industry, with Chalmers University of Technology, Gothenburg University and Malm{\"{o}} University as academic partners and Ericsson, AB Volvo, Volvo Car Corporation, Saab Electronic Defense Systems, Grundfos, Axis Communications, Jeppesen (Boeing) and Sony Mobile as industrial partners. The 17 chapters present the "Stairway to Heaven" model, which represents the typical evolution path companies move through as they develop and mature their software engineering capabilities. The chapters describe theoretical frameworks, conceptual models and, most importantly, the industrial experiences gained by the partner companies in applying novel software engineering techniques. The book's structure consists of six parts. Part I describes the model in detail and presents an overview of lessons learned in the collaboration between industry and academia. Part II deals with the first step of the Stairway to Heaven, in which R{\&}D adopts agile work practices. Part III of the book combines the next two phases, i.e., continuous integration (CI) and continuous delivery (CD), as they are closely intertwined. Part IV is concerned with the highest level, referred to as "R{\&}D as an innovation system," while Part V addresses a topic that is separate from the Stairway to Heaven and yet critically important in large organizations: organizational performance metrics that capture data, and visualizations of the status of software assets, defects and teams. Lastly, Part VI presents the perspectives of two of the SC partner companies. The book is intended for practitioners and professionals in the software-intensive systems industry, providing concrete models, frameworks and case studies that show the specific challenges that the partner companies encountered, their approaches to overcoming them, and the results. Researchers will gain valuable insights on the problems faced by large software companies, and on how to effectively tackle them in the context of successful cooperation projects.},
author = {Bosch, Jan},
booktitle = {Continuous software engineering},
doi = {10.1007/978-3-319-11283-1},
file = {:Users/jk/Documents/Uni/master/refs/bosch2012CSE.pdf:pdf},
isbn = {9783319112831},
mendeley-groups = {Master Thesis},
pages = {1--226},
title = {{Continuous software engineering}},
volume = {9783319112},
year = {2014}
}
@article{Fitzgerald2017,
abstract = {Throughout its short history, software development has been characterized by harmful disconnects between important activities such as planning, development and implementation. The problem is further exacerbated by the episodic and infrequent performance of activities such as planning, testing, integration and releases. Several emerging phenomena reflect attempts to address these problems. For example, Continuous Integration is a practice which has emerged to eliminate discontinuities between development and deployment. In a similar vein, the recent emphasis on DevOps recognizes that the integration between software development and its operational deployment needs to be a continuous one. We argue a similar continuity is required between business strategy and development, BizDev being the term we coin for this. These disconnects are even more problematic given the need for reliability and resilience in the complex and data-intensive systems being developed today. We identify a number of continuous activities which together we label as ‘Continuous*' (i.e. Continuous Star) which we present as part of an overall roadmap for Continuous Software engineering. We argue for a continuous (but not necessarily rapid) software engineering delivery pipeline. We conclude the paper with a research agenda.},
archivePrefix = {arXiv},
arxivId = {01641212},
author = {Fitzgerald, Brian and Stol, Klaas Jan},
doi = {10.1016/j.jss.2015.06.063},
eprint = {01641212},
file = {:Users/jk/Documents/Uni/master/refs/fitzgerald2015continuous.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Continuous software engineering,DevOps,Lean software development},
mendeley-groups = {Master Thesis},
pages = {176--189},
publisher = {Elsevier Ltd.},
title = {{Continuous software engineering: A roadmap and agenda}},
url = {http://dx.doi.org/10.1016/j.jss.2015.06.063},
volume = {123},
year = {2017}
}
@online{WEB:Fowler:2005-3,
	Author = {Fowler, Martin},
	Title = {Retroactive Event},
	Url = {https://martinfowler.com/eaaDev/RetroactiveEvent.html},
	Urldate = {2017-11-26},
	Year = {2005}
}
@article{strauch2011nosql,
  title={NoSQL databases},
  author={Strauch, Christof and Sites, Ultra-Large Scale and Kriha, Walter},
  journal={Lecture Notes, Stuttgart Media University},
  volume={20},
  year={2011}
}@article{strauch2011nosql,
  title={NoSQL databases},
  author={Strauch, Christof and Sites, Ultra-Large Scale and Kriha, Walter},
  journal={Lecture Notes, Stuttgart Media University},
  volume={20},
  year={2011}
}@INPROCEEDINGS{6625441,
author={Y. Li and S. Manoharan},
booktitle={2013 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
title={A performance comparison of SQL and NoSQL databases},
year={2013},
volume={},
number={},
pages={15-19},
keywords={SQL;NoSQL databases;SQL databases;abstract key-value pair framework;big data;delete operations;instantiate operations;key-value stores;read operations;write operations;Abstracts;Data handling;Data models;Databases;Information management;Media;Time measurement;Database performance;NoSQL databases;SQL},
doi={10.1109/PACRIM.2013.6625441},
ISSN={1555-5798},
month={Aug},}